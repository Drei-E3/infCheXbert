{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Capture Output v0.0.11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jupyter_capture_output\n",
    "working_path = os.getcwd()\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "from uer.utils.vocab import Vocab\n",
    "from uer.utils.constants import *\n",
    "from uer.utils.tokenizer import * \n",
    "from uer.model_builder import build_model\n",
    "from uer.utils.optimizers import  BertAdam\n",
    "from uer.utils.config import load_hyperparam\n",
    "from uer.utils.seed import set_seed\n",
    "from uer.model_saver import save_model\n",
    "from brain import KnowledgeGraph\n",
    "from multiprocessing import Process, Pool\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from inf_classifier import BertClassifier, add_argument_for_paser_of_BertClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "add_argument_for_paser_of_BertClassifier(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=parser.parse_args([\n",
    "  \"--pretrained_model_path\", \"./models/google_model.bin\" ,\n",
    "    \"--config_path\", \"./models/google_config.json\", \n",
    "    \"--vocab_path\", \"./models/google_vocab.txt\", \n",
    "    \"--train_path\", \"./datasets/CheXpert/impression/train.csv\", \n",
    "    \"--dev_path\", \"./datasets/CheXpert/impression/validation.csv\",\n",
    "    \"--test_path\", \"./datasets/CheXpert/impression/test.csv\",\n",
    "    \"--epochs_num\", \"3\", \n",
    "    \"--batch_size\", \"16\", \n",
    "   #  \"--tokenizer_from_huggingface\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    \"--tokenizer_from_huggingface\", \"\",\n",
    "    \"--kg_name\", \"./brain/kgs/CheXpert_KG.spo\",\n",
    "    \"--output_model_path\", \"./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated.bin\"])\n",
    "\n",
    "\n",
    "args = load_hyperparam(args)\n",
    "args.learning_rate = 2e-5\n",
    "set_seed(args.seed)\n",
    "\n",
    "str2tokenizer = {\"char\": CharTokenizer, \"space\": SpaceTokenizer, \"bert\": BertTokenizer}\n",
    "args.tokenizer = str2tokenizer[args.tokenizer](args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '.bin' not in args.output_model_path:\n",
    "    print('output models should be saved as .bin file')\n",
    "else:\n",
    "    path = args.output_model_path.split('/')\n",
    "    name_of_modelscompany = path[-1].replace('.bin','')\n",
    "    path = [x + '/' for x in path]\n",
    "    path.remove(path[-1])\n",
    "    Parent_directory = ''.join(path)\n",
    "    if not os.path.exists(Parent_directory):\n",
    "        os.makedirs(Parent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialise global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for english sentences, huggingface tokenizer or nltk tokenizer is recommanded\n",
      "Vocabulary file line 344 has bad format token\n",
      "Vocabulary Size:  21128\n",
      "labeler label_Atelectasis would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Atelectasis.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Cardiomegaly would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Cardiomegaly.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Consolidation would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Consolidation.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Edema would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Edema.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Enlarged Cardiomediastinum would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Enlarged Cardiomediastinum.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Fracture would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Fracture.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Lung Lesion would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Lung Lesion.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Lung Opacity would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Lung Opacity.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_No Finding would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_No Finding.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Pleural Effusion would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Pleural Effusion.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Pleural Other would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Pleural Other.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Pneumonia would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Pneumonia.bin\n",
      "[BertClassifier] use visible_matrix: True\n",
      "labeler label_Pneumothorax would be saved under path ./outputs/infCheXbert_half_integrated/infCheXbert_half_integrated_label_Pneumothorax.bin\n",
      "[BertClassifier] use visible_matrix: True\n"
     ]
    }
   ],
   "source": [
    "columns = {} # to check column locations of labels and text \n",
    "label_columns = {} # to check column location of each label\n",
    "label_names = []\n",
    "labels_sets= [] # a list of store sets of label values, in my case, label values are 0,1,2\n",
    "with open(args.train_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line_id, line in enumerate(f):\n",
    "        try:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line_id == 0:\n",
    "                for i, column_name in enumerate(line):\n",
    "                    columns[column_name] = i\n",
    "                    if 'label' in column_name:\n",
    "                        label_columns[column_name] = i\n",
    "                        label_names.append( column_name)\n",
    "                        labels_sets.append(set())\n",
    "                continue\n",
    "            # count label numbers for each label name\n",
    "            # in our case, labels_nums is known: 3 for each label name. following is just for generalization\n",
    "            for i,label_set in enumerate(labels_sets):\n",
    "                label = int(line[label_columns[label_names[i]]])\n",
    "                label_set.add(label)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "labels_nums = [len(labels_set) for labels_set in labels_sets] \n",
    "\n",
    "# Load vocabulary.\n",
    "vocab = Vocab()\n",
    "print('for english sentences, huggingface tokenizer or nltk tokenizer is recommanded')\n",
    "if args.tokenizer_from_huggingface: \n",
    "    print('huggingface tokenizer detected, use tokenizer {args.tokenizer_from_huggingface}')\n",
    "    tok_en = AutoTokenizer.from_pretrained(args.tokenizer_from_huggingface)\n",
    "vocab.load(args.vocab_path)\n",
    "args.vocab = vocab\n",
    "args.target = \"bert\"\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() \n",
    "                      else \"cuda\" if torch.cuda.is_available() \n",
    "                      else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available(): torch.mps.empty_cache()\n",
    "# Build bert model.\n",
    "models = []\n",
    "output_model_path = []\n",
    "\n",
    "for counter,label_num in enumerate(labels_nums):\n",
    "    args.labels_num = label_num\n",
    "    output_model_path.append(args.output_model_path.replace('.bin','') + '_' + label_names[counter] +'.bin')\n",
    "    print(f'labeler {label_names[counter]} would be saved under path {output_model_path[counter]}')\n",
    "    model = build_model(args)\n",
    "    # Load or initialize parameters.\n",
    "    if args.pretrained_model_path is not None:\n",
    "        # Initialize with pretrained model.\n",
    "        model.load_state_dict(torch.load(args.pretrained_model_path), strict=False)  \n",
    "    else:\n",
    "        # Initialize with normal distribution.\n",
    "        for n, p in list(model.named_parameters()):\n",
    "            if 'gamma' not in n and 'beta' not in n:\n",
    "                p.data.normal_(0, 0.02)\n",
    "\n",
    "    model = BertClassifier(args, model)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"{} GPUs are available. Let's use them.\".format(torch.cuda.device_count()))\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "args.output_model_path = output_model_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KnowledgeGraph] Loading spo from ./brain/kgs/CheXpert_KG.spo\n"
     ]
    }
   ],
   "source": [
    "if args.kg_name == 'none':\n",
    "    spo_files = []\n",
    "else:\n",
    "    spo_files = [args.kg_name]\n",
    "kg = KnowledgeGraph(spo_files=spo_files, tokenizer=args.tokenizer,predicate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define assist functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_knowledge_worker(params):\n",
    "    '''\n",
    "    - input parameters are p_id, sentences, columns, label_columns, kg, vocab, args.seq_length\n",
    "    - output is a dataset which is a list\n",
    "        - structure of the dataset output:\n",
    "            output[0]=token_ids, \n",
    "            output[1]=label, \n",
    "            output[2]=mask,\n",
    "            output[3]=pos, \n",
    "            output[4]=vm \n",
    "        - in case of multiple classifier with multiple labels: output[1]=label is again a list\n",
    "            where each entry has all label value for its label name\n",
    "            for example: \n",
    "                output[1][0] == [1,0,2,3,4,...] for columns['label_Atelectasis]\n",
    "                output[1][1] == [1,0,2,3,4,...] for columns['label_Cardiomegaly]\n",
    "        \n",
    "\n",
    "    '''\n",
    "\n",
    "    p_id, sentences, columns, kg, vocab, args = params\n",
    "    text_column = {}\n",
    "    labels_columns = {}\n",
    "    for k,v in columns.items():\n",
    "        if 'label' in k:\n",
    "            labels_columns.update({k:v})\n",
    "        else:\n",
    "            text_column.update({k:v})\n",
    "\n",
    "    sentences_num = len(sentences)\n",
    "    dataset = []\n",
    "\n",
    "    labels_position = list(labels_columns.values())\n",
    "    text_position = list(text_column.values())\n",
    "\n",
    "    for line_id, line in enumerate(sentences):\n",
    "        if line_id % 10000 == 0:\n",
    "            print(\"Progress of process {}: {}/{}\".format(p_id, line_id, sentences_num))\n",
    "            sys.stdout.flush()\n",
    "        line = line.strip().split('\\t')\n",
    "        try:\n",
    "            if len(line) == 2:\n",
    "                label = [int(line[columns[\"label\"]])]\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]]\n",
    "   \n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "                if args.tokenizer_from_huggingface:\n",
    "                    token_ids = tok_en.convert_tokens_to_ids(tokens)\n",
    "                else:\n",
    "                    token_ids = [vocab.get(t) for t in tokens]\n",
    "                    # token_ids = args.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                \n",
    "                mask = [1 if t != PAD_TOKEN else 0 for t in tokens]\n",
    "\n",
    "                dataset.append((token_ids, label, mask, pos, vm))\n",
    "            \n",
    "            elif (len(line) == 3) and (\"text_b\" in line):\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]] + SEP_TOKEN + line[columns[\"text_b\"]] + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = []\n",
    "                seg_tag = 1\n",
    "                for t in tokens:\n",
    "                    if t == PAD_TOKEN:\n",
    "                        mask.append(0)\n",
    "                    else:\n",
    "                        mask.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "\n",
    "                dataset.append((token_ids, label, mask, pos, vm))\n",
    "            \n",
    "            elif (len(line) == 4) and ('qid' in line):  # for dbqa\n",
    "                qid=int(line[columns[\"qid\"]])\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n",
    "                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = []\n",
    "                seg_tag = 1\n",
    "                for t in tokens:\n",
    "                    if t == PAD_TOKEN:\n",
    "                        mask.append(0)\n",
    "                    else:\n",
    "                        mask.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "                \n",
    "                dataset.append((token_ids, label, mask, pos, vm, qid))\n",
    "\n",
    "            # multiple classification with multiple labels\n",
    "\n",
    "            elif len(labels_columns.keys()) >=2 : \n",
    "\n",
    "                labels = [line[x] for x in labels_position]\n",
    "                text = [line[x] for x in text_position]\n",
    "                text = CLS_TOKEN + ' ' +text[0]\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "                # token_ids = [vocab.get(t) for t in tokens]\n",
    "                token_ids = args.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                # token_ids = tok_en.convert_tokens_to_ids(tokens)\n",
    "                mask = [1 if t != PAD_TOKEN else 0 for t in tokens]\n",
    "\n",
    "                dataset.append((token_ids, labels, mask, pos, vm))\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error line: \", line)\n",
    "            print(e)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch loader.\n",
    "\n",
    "'''\n",
    "def batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        labels_ids_batch = [label_id[i*batch_size: (i+1)*batch_size] for label_id in label_ids]\n",
    "        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        labels_ids_batch = [label_id[instances_num//batch_size*batch_size:] for label_id in label_ids]\n",
    "        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "\n",
    "        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "'''\n",
    "\n",
    "def multi_label_batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        labels_ids_batch = [label_id[i*batch_size: (i+1)*batch_size] for label_id in label_ids]\n",
    "        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        labels_ids_batch = [label_id[instances_num//batch_size*batch_size:] for label_id in label_ids]\n",
    "        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "def batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size]\n",
    "        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n",
    "        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "# dataset loader\n",
    "def read_dataset(path, workers_num=1):\n",
    "\n",
    "    print(\"Loading sentences from {}\".format(path))\n",
    "    sentences = []\n",
    "    with open(path, mode='r', encoding=\"utf-8\") as f:\n",
    "        for line_id, line in enumerate(f):\n",
    "            if line_id == 0:\n",
    "                continue\n",
    "            sentences.append(line)\n",
    "    sentence_num = len(sentences)\n",
    "\n",
    "    print(\"There are {} sentence in total. We use {} processes to inject knowledge into sentences.\".format(sentence_num, workers_num))\n",
    "    if workers_num > 1:\n",
    "        params = []\n",
    "        sentence_per_block = int(sentence_num / workers_num) + 1\n",
    "        for i in range(workers_num):\n",
    "            params.append((i, sentences[i*sentence_per_block: (i+1)*sentence_per_block], columns, kg, vocab, args))\n",
    "        pool = Pool(workers_num)\n",
    "        res = pool.map(add_knowledge_worker, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        dataset = [sample for block in res for sample in block]\n",
    "    else:\n",
    "        params = (0, sentences, columns, kg, vocab, args)\n",
    "        dataset = add_knowledge_worker(params)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Evaluation function.\n",
    "def evaluate(args, is_test, metrics='Acc',label_id = -1):\n",
    "    counter = label_id\n",
    "    if is_test:\n",
    "        dataset = read_dataset(args.test_path, workers_num=args.workers_num)\n",
    "    else:\n",
    "        dataset = read_dataset(args.dev_path, workers_num=args.workers_num)\n",
    "    labels_ids=[]\n",
    "    input_ids = torch.LongTensor([sample[0] for sample in dataset])\n",
    "    if label_id == -1:\n",
    "        label_ids = torch.LongTensor([sample[1] for sample in dataset])\n",
    "    for nr, label in enumerate(label_columns):\n",
    "        labels_ids.append(torch.LongTensor([int(example[1][nr]) for example in dataset]))\n",
    "    mask_ids = torch.LongTensor([sample[2] for sample in dataset])\n",
    "    pos_ids = torch.LongTensor([example[3] for example in dataset])\n",
    "    vms = [example[4] for example in dataset]\n",
    "    label_ids=labels_ids[counter]\n",
    "    batch_size = args.batch_size\n",
    "    instances_num = input_ids.size()[0]\n",
    "    if is_test:\n",
    "        print(\"The number of evaluation instances: \", instances_num)\n",
    "\n",
    "    correct = 0\n",
    "    # Confusion matrix.\n",
    "\n",
    "    confusions = [torch.zeros(num, num, dtype=torch.long) for num in labels_nums]\n",
    "    confusion = confusions[counter]\n",
    "    model = models[counter]\n",
    "    label_name =label_names[counter]\n",
    "    label_ids = labels_ids[counter]\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if not args.mean_reciprocal_rank:\n",
    "        for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "\n",
    "            # vms_batch = vms_batch.long()\n",
    "            vms_batch = torch.LongTensor(np.array(vms_batch))\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            mask_ids_batch = mask_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                try:\n",
    "                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n",
    "                except:\n",
    "                    print(input_ids_batch)\n",
    "                    print(input_ids_batch.size())\n",
    "                    print(vms_batch)\n",
    "                    print(vms_batch.size())\n",
    "\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            pred = torch.argmax(logits, dim=1).to(device)\n",
    "            gold = label_ids_batch\n",
    "\n",
    "            for j in range(pred.size()[0]):\n",
    "                    confusion[pred[j], gold[j]] += 1\n",
    "            correct += torch.sum(pred == gold).item()\n",
    "    \n",
    "        if is_test:\n",
    "            print(\"Confusion matrix of {label_name}:\")\n",
    "            print(confusion)\n",
    "            print(\"Report precision, recall, and f1:\")\n",
    "        \n",
    "        for i in range(confusion.size()[0]):\n",
    "\n",
    "            if (confusion[i,:].sum().item() == 0) and (confusion[:,i].sum().item()!= 0):\n",
    "                print(f'model never predicts label value {i}')\n",
    "\n",
    "\n",
    "            elif (confusion[:,i].sum().item()== 0) and (confusion[i,:].sum().item() != 0):\n",
    "                print(f'dataset has no label value {i}')\n",
    "            eps = 1e-9\n",
    "            p = confusion[i,i].item()/(confusion[i,:].sum().item() + eps)\n",
    "            r = confusion[i,i].item()/(confusion[i,:].sum().item() + eps)\n",
    "            f1 = 2*p*r / (p + r + eps)\n",
    "\n",
    "            if i == 1:\n",
    "                label_1_f1 = f1\n",
    "            print(\"labelsname: {}, Label_value {}: {:.3f}, {:.3f}, {:.3f}\".format(label_name,i,p,r,f1))\n",
    "        print(\"Acc. (Correct/Total): {:.4f} ({}/{}) \".format(correct/len(dataset), correct, len(dataset)))\n",
    "        if metrics == 'Acc':\n",
    "            return correct/len(dataset)\n",
    "        elif metrics == 'f1':\n",
    "            return label_1_f1\n",
    "        else:\n",
    "            return correct/len(dataset)\n",
    "    else:\n",
    "            for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "\n",
    "                vms_batch = torch.LongTensor(np.array(vms_batch))\n",
    "\n",
    "                input_ids_batch = input_ids_batch.to(device)\n",
    "                label_ids_batch = label_ids_batch.to(device)\n",
    "                mask_ids_batch = mask_ids_batch.to(device)\n",
    "                pos_ids_batch = pos_ids_batch.to(device)\n",
    "                vms_batch = vms_batch.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n",
    "                logits = nn.Softmax(dim=1)(logits)\n",
    "                if i == 0:\n",
    "                    logits_all=logits\n",
    "                if i >= 1:\n",
    "                    logits_all=torch.cat((logits_all,logits),0)\n",
    "        \n",
    "            order = -1\n",
    "            gold = []\n",
    "            for i in range(len(dataset)):\n",
    "                qid = dataset[i][-1]\n",
    "                label = dataset[i][1]\n",
    "                if qid == order:\n",
    "                    j += 1\n",
    "                    if label == 1:\n",
    "                        gold.append((qid,j))\n",
    "                else:\n",
    "                    order = qid\n",
    "                    j = 0\n",
    "                    if label == 1:\n",
    "                        gold.append((qid,j))\n",
    "\n",
    "            label_order = []\n",
    "            order = -1\n",
    "            for i in range(len(gold)):\n",
    "                if gold[i][0] == order:\n",
    "                    templist.append(gold[i][1])\n",
    "                elif gold[i][0] != order:\n",
    "                    order=gold[i][0]\n",
    "                    if i > 0:\n",
    "                        label_order.append(templist)\n",
    "                    templist = []\n",
    "                    templist.append(gold[i][1])\n",
    "            label_order.append(templist)\n",
    "\n",
    "            order = -1\n",
    "            score_list = []\n",
    "            for i in range(len(logits_all)):\n",
    "                score = float(logits_all[i][1])\n",
    "                qid=int(dataset[i][-1])\n",
    "                if qid == order:\n",
    "                    templist.append(score)\n",
    "                else:\n",
    "                    order = qid\n",
    "                    if i > 0:\n",
    "                        score_list.append(templist)\n",
    "                    templist = []\n",
    "                    templist.append(score)\n",
    "            score_list.append(templist)\n",
    "\n",
    "            rank = []\n",
    "            pred = []\n",
    "            print(len(score_list))\n",
    "            print(len(label_order))\n",
    "            for i in range(len(score_list)):\n",
    "                if len(label_order[i])==1:\n",
    "                    if label_order[i][0] < len(score_list[i]):\n",
    "                        true_score = score_list[i][label_order[i][0]]\n",
    "                        score_list[i].sort(reverse=True)\n",
    "                        for j in range(len(score_list[i])):\n",
    "                            if score_list[i][j] == true_score:\n",
    "                                rank.append(1 / (j + 1))\n",
    "                    else:\n",
    "                        rank.append(0)\n",
    "\n",
    "                else:\n",
    "                    true_rank = len(score_list[i])\n",
    "                    for k in range(len(label_order[i])):\n",
    "                        if label_order[i][k] < len(score_list[i]):\n",
    "                            true_score = score_list[i][label_order[i][k]]\n",
    "                            temp = sorted(score_list[i],reverse=True)\n",
    "                            for j in range(len(temp)):\n",
    "                                if temp[j] == true_score:\n",
    "                                    if j < true_rank:\n",
    "                                        true_rank = j\n",
    "                    if true_rank < len(score_list[i]):\n",
    "                        rank.append(1 / (true_rank + 1))\n",
    "                    else:\n",
    "                        rank.append(0)\n",
    "            MRR = sum(rank) / len(rank)\n",
    "            print(\"MRR\", MRR)\n",
    "            return MRR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load datasets, integrate KG into datasets and convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load training dataset\n",
      "Loading sentences from ./datasets/CheXpert/impression/train.csv\n",
      "There are 99371 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/99371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Training phase.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mload training dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m trainset \u001b[39m=\u001b[39m read_dataset(args\u001b[39m.\u001b[39;49mtrain_path, workers_num\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mworkers_num)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShuffling dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m random\u001b[39m.\u001b[39mshuffle(trainset)\n",
      "Cell \u001b[0;32mIn[13], line 81\u001b[0m, in \u001b[0;36mread_dataset\u001b[0;34m(path, workers_num)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     params \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, sentences, columns, kg, vocab, args)\n\u001b[0;32m---> 81\u001b[0m     dataset \u001b[39m=\u001b[39m add_knowledge_worker(params)\n\u001b[1;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[12], line 113\u001b[0m, in \u001b[0;36madd_knowledge_worker\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    110\u001b[0m text \u001b[39m=\u001b[39m [line[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m text_position]\n\u001b[1;32m    111\u001b[0m text \u001b[39m=\u001b[39m CLS_TOKEN \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39mtext[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 113\u001b[0m tokens, pos, vm, _ \u001b[39m=\u001b[39m kg\u001b[39m.\u001b[39;49madd_knowledge_with_vm([text], add_pad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mseq_length)\n\u001b[1;32m    114\u001b[0m tokens \u001b[39m=\u001b[39m tokens[\u001b[39m0\u001b[39m]\n\u001b[1;32m    115\u001b[0m pos \u001b[39m=\u001b[39m pos[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/BA/infCheXbert_with_gewighted_f1/brain/knowgraph.py:144\u001b[0m, in \u001b[0;36mKnowledgeGraph.add_knowledge_with_vm\u001b[0;34m(self, sent_batch, max_entities, add_pad, max_length)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m split_sent:\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_matching:\n\u001b[0;32m--> 144\u001b[0m         keys_to_match \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlookup_table\u001b[39m.\u001b[39;49mkeys() \u001b[39mif\u001b[39;49;00m x\u001b[39m.\u001b[39;49mstartswith(token)]\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m keys_to_match: \u001b[39m# start matching\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         token_group\u001b[39m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/Documents/BA/infCheXbert_with_gewighted_f1/brain/knowgraph.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m split_sent:\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_matching:\n\u001b[0;32m--> 144\u001b[0m         keys_to_match \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_table\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39;49mstartswith(token)]\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m keys_to_match: \u001b[39m# start matching\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         token_group\u001b[39m.\u001b[39mappend(token)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training phase.\n",
    "print(\"load training dataset\")\n",
    "trainset = read_dataset(args.train_path, workers_num=args.workers_num)\n",
    "print(\"Shuffling dataset\")\n",
    "random.shuffle(trainset)\n",
    "instances_num = len(trainset)\n",
    "batch_size = args.batch_size\n",
    "\n",
    "print(\"Transfer data to tensor, which includes: \")\n",
    "print(\"input_ids\")\n",
    "input_ids = torch.LongTensor([example[0] for example in trainset])\n",
    "print(\"label_ids\")\n",
    "labels_ids = []\n",
    "\n",
    "for counter in range(len(label_columns)):\n",
    "    labels_ids.append(torch.LongTensor([int(example[1][counter]) for example in trainset]))\n",
    "\n",
    "'''\n",
    "for counter, label in enumerate(label_columns):\n",
    "    # labels_ids.append(torch.LongTensor([int(example[1][label_columns[label]]) for example in trainset]))\n",
    "    labels_ids.append(torch.LongTensor([int(example[1][counter]) for example in trainset]))\n",
    "'''\n",
    "print(\"mask_ids\")\n",
    "mask_ids = torch.LongTensor([example[2] for example in trainset])\n",
    "print(\"pos_ids\")\n",
    "pos_ids = torch.LongTensor([example[3] for example in trainset])\n",
    "print(\"vms\")\n",
    "vms = [example[4] for example in trainset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelscompany for 3 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_3_obs = models[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  16\n",
      "The number of training instances: 99371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zm_leng/Documents/BA/infCheXbert_with_gewighted_f1/uer/utils/optimizers.py:123: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 100, Avg loss: 1.052\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 200, Avg loss: 0.724\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 300, Avg loss: 0.615\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 400, Avg loss: 0.487\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 500, Avg loss: 0.403\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 600, Avg loss: 0.318\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 700, Avg loss: 0.349\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 800, Avg loss: 0.284\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 900, Avg loss: 0.263\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1000, Avg loss: 0.283\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1100, Avg loss: 0.233\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1200, Avg loss: 0.308\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1300, Avg loss: 0.253\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1400, Avg loss: 0.223\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1500, Avg loss: 0.272\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1600, Avg loss: 0.251\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1700, Avg loss: 0.246\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1800, Avg loss: 0.252\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1900, Avg loss: 0.293\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2000, Avg loss: 0.249\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2100, Avg loss: 0.253\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2200, Avg loss: 0.278\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2300, Avg loss: 0.238\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2400, Avg loss: 0.259\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2500, Avg loss: 0.249\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2600, Avg loss: 0.236\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2700, Avg loss: 0.292\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2800, Avg loss: 0.253\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2900, Avg loss: 0.241\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3000, Avg loss: 0.253\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3100, Avg loss: 0.298\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3200, Avg loss: 0.260\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3300, Avg loss: 0.252\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3400, Avg loss: 0.231\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3500, Avg loss: 0.259\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3600, Avg loss: 0.269\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3700, Avg loss: 0.267\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3800, Avg loss: 0.220\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3900, Avg loss: 0.240\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4000, Avg loss: 0.213\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4100, Avg loss: 0.228\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4200, Avg loss: 0.202\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4300, Avg loss: 0.235\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4400, Avg loss: 0.180\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4500, Avg loss: 0.220\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4600, Avg loss: 0.213\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4700, Avg loss: 0.243\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4800, Avg loss: 0.194\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4900, Avg loss: 0.188\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5000, Avg loss: 0.189\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5100, Avg loss: 0.235\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5200, Avg loss: 0.163\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5300, Avg loss: 0.186\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5400, Avg loss: 0.210\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5500, Avg loss: 0.269\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5600, Avg loss: 0.220\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5700, Avg loss: 0.189\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5800, Avg loss: 0.206\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5900, Avg loss: 0.225\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 6000, Avg loss: 0.197\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 6100, Avg loss: 0.201\n",
      "labelsname: label_Atelectasis, Epoch id: 1, Training steps: 6200, Avg loss: 0.175\n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/validation.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.984, 0.984, 0.984\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.774, 0.774, 0.774\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.000, 0.000, 0.000\n",
      "Acc. (Correct/Total): 0.9330 (30904/33124) \n",
      "Start evaluation on evaluation dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/test.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "The number of evaluation instances:  33124\n",
      "Confusion matrix of {label_name}:\n",
      "tensor([[24698,   331,     8,    75],\n",
      "        [   15,  6193,   202,  1601],\n",
      "        [    0,     0,     0,     0],\n",
      "        [    0,     0,     0,     1]])\n",
      "Report precision, recall, and f1:\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.984, 0.984, 0.984\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.773, 0.773, 0.773\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 1.000, 1.000, 1.000\n",
      "Acc. (Correct/Total): 0.9326 (30892/33124) \n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 100, Avg loss: 0.203\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 200, Avg loss: 0.177\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 300, Avg loss: 0.218\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 400, Avg loss: 0.218\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 500, Avg loss: 0.207\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 600, Avg loss: 0.185\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 700, Avg loss: 0.209\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 800, Avg loss: 0.180\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 900, Avg loss: 0.192\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1000, Avg loss: 0.187\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1100, Avg loss: 0.176\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1200, Avg loss: 0.225\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1300, Avg loss: 0.180\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1400, Avg loss: 0.183\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1500, Avg loss: 0.226\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1600, Avg loss: 0.200\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1700, Avg loss: 0.182\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1800, Avg loss: 0.186\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1900, Avg loss: 0.220\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2000, Avg loss: 0.168\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2100, Avg loss: 0.198\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2200, Avg loss: 0.188\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2300, Avg loss: 0.151\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2400, Avg loss: 0.180\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2500, Avg loss: 0.182\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2600, Avg loss: 0.168\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2700, Avg loss: 0.225\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2800, Avg loss: 0.182\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2900, Avg loss: 0.156\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3000, Avg loss: 0.173\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3100, Avg loss: 0.191\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3200, Avg loss: 0.201\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3300, Avg loss: 0.182\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3400, Avg loss: 0.177\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3500, Avg loss: 0.189\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3600, Avg loss: 0.200\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3700, Avg loss: 0.197\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3800, Avg loss: 0.186\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3900, Avg loss: 0.184\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4000, Avg loss: 0.172\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4100, Avg loss: 0.176\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4200, Avg loss: 0.173\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4300, Avg loss: 0.194\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4400, Avg loss: 0.150\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4500, Avg loss: 0.182\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4600, Avg loss: 0.177\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4700, Avg loss: 0.203\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4800, Avg loss: 0.150\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4900, Avg loss: 0.158\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5000, Avg loss: 0.160\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5100, Avg loss: 0.202\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5200, Avg loss: 0.138\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5300, Avg loss: 0.159\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5400, Avg loss: 0.189\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5500, Avg loss: 0.235\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5600, Avg loss: 0.182\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5700, Avg loss: 0.147\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5800, Avg loss: 0.172\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5900, Avg loss: 0.207\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 6000, Avg loss: 0.167\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 6100, Avg loss: 0.178\n",
      "labelsname: label_Atelectasis, Epoch id: 2, Training steps: 6200, Avg loss: 0.157\n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/validation.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.985, 0.985, 0.985\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.857, 0.857, 0.857\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.676, 0.676, 0.676\n",
      "Acc. (Correct/Total): 0.9475 (31384/33124) \n",
      "Start evaluation on evaluation dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/test.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "The number of evaluation instances:  33124\n",
      "Confusion matrix of {label_name}:\n",
      "tensor([[24688,   294,     8,    71],\n",
      "        [   22,  5856,   188,   734],\n",
      "        [    0,     0,     0,     0],\n",
      "        [    3,   374,    14,   872]])\n",
      "Report precision, recall, and f1:\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.985, 0.985, 0.985\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.861, 0.861, 0.861\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.690, 0.690, 0.690\n",
      "Acc. (Correct/Total): 0.9484 (31416/33124) \n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 100, Avg loss: 0.177\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 200, Avg loss: 0.166\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 300, Avg loss: 0.204\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 400, Avg loss: 0.198\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 500, Avg loss: 0.190\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 600, Avg loss: 0.160\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 700, Avg loss: 0.190\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 800, Avg loss: 0.161\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 900, Avg loss: 0.151\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1000, Avg loss: 0.168\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1100, Avg loss: 0.160\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1200, Avg loss: 0.215\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1300, Avg loss: 0.156\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1400, Avg loss: 0.149\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1500, Avg loss: 0.213\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1600, Avg loss: 0.189\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1700, Avg loss: 0.168\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1800, Avg loss: 0.170\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1900, Avg loss: 0.204\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2000, Avg loss: 0.155\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2100, Avg loss: 0.207\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2200, Avg loss: 0.175\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2300, Avg loss: 0.140\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2400, Avg loss: 0.166\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2500, Avg loss: 0.179\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2600, Avg loss: 0.166\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2700, Avg loss: 0.215\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2800, Avg loss: 0.176\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2900, Avg loss: 0.149\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3000, Avg loss: 0.161\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3100, Avg loss: 0.176\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3200, Avg loss: 0.197\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3300, Avg loss: 0.181\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3400, Avg loss: 0.174\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3500, Avg loss: 0.185\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3600, Avg loss: 0.198\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3700, Avg loss: 0.199\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3800, Avg loss: 0.182\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3900, Avg loss: 0.183\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4000, Avg loss: 0.163\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4100, Avg loss: 0.172\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4200, Avg loss: 0.164\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4300, Avg loss: 0.178\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4400, Avg loss: 0.138\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4500, Avg loss: 0.181\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4600, Avg loss: 0.171\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4700, Avg loss: 0.202\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4800, Avg loss: 0.143\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4900, Avg loss: 0.149\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5000, Avg loss: 0.148\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5100, Avg loss: 0.196\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5200, Avg loss: 0.121\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5300, Avg loss: 0.156\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5400, Avg loss: 0.184\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5500, Avg loss: 0.222\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5600, Avg loss: 0.173\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5700, Avg loss: 0.149\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5800, Avg loss: 0.170\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5900, Avg loss: 0.197\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 6000, Avg loss: 0.164\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 6100, Avg loss: 0.174\n",
      "labelsname: label_Atelectasis, Epoch id: 3, Training steps: 6200, Avg loss: 0.151\n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/validation.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.985, 0.985, 0.985\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.856, 0.856, 0.856\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.715, 0.715, 0.715\n",
      "Acc. (Correct/Total): 0.9489 (31432/33124) \n",
      "Start evaluation on evaluation dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/test.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "The number of evaluation instances:  33124\n",
      "Confusion matrix of {label_name}:\n",
      "tensor([[24692,   302,     8,    70],\n",
      "        [   19,  5918,   190,   785],\n",
      "        [    0,     0,     0,     0],\n",
      "        [    2,   304,    12,   822]])\n",
      "Report precision, recall, and f1:\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.985, 0.985, 0.985\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.856, 0.856, 0.856\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.721, 0.721, 0.721\n",
      "Acc. (Correct/Total): 0.9489 (31432/33124) \n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 100, Avg loss: 0.170\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 200, Avg loss: 0.147\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 300, Avg loss: 0.192\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 400, Avg loss: 0.205\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 500, Avg loss: 0.185\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 600, Avg loss: 0.157\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 700, Avg loss: 0.177\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 800, Avg loss: 0.159\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 900, Avg loss: 0.151\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1000, Avg loss: 0.158\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1100, Avg loss: 0.150\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1200, Avg loss: 0.201\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1300, Avg loss: 0.147\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1400, Avg loss: 0.147\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1500, Avg loss: 0.199\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1600, Avg loss: 0.174\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1700, Avg loss: 0.163\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1800, Avg loss: 0.159\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 1900, Avg loss: 0.193\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2000, Avg loss: 0.153\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2100, Avg loss: 0.179\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2200, Avg loss: 0.159\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2300, Avg loss: 0.126\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2400, Avg loss: 0.159\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2500, Avg loss: 0.165\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2600, Avg loss: 0.157\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2700, Avg loss: 0.199\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2800, Avg loss: 0.170\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 2900, Avg loss: 0.140\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3000, Avg loss: 0.155\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3100, Avg loss: 0.170\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3200, Avg loss: 0.179\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3300, Avg loss: 0.175\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3400, Avg loss: 0.163\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3500, Avg loss: 0.171\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3600, Avg loss: 0.176\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3700, Avg loss: 0.187\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3800, Avg loss: 0.175\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 3900, Avg loss: 0.169\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4000, Avg loss: 0.163\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4100, Avg loss: 0.164\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4200, Avg loss: 0.155\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4300, Avg loss: 0.164\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4400, Avg loss: 0.140\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4500, Avg loss: 0.175\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4600, Avg loss: 0.164\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4700, Avg loss: 0.192\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4800, Avg loss: 0.134\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 4900, Avg loss: 0.146\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5000, Avg loss: 0.147\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5100, Avg loss: 0.186\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5200, Avg loss: 0.122\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5300, Avg loss: 0.142\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5400, Avg loss: 0.172\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5500, Avg loss: 0.205\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5600, Avg loss: 0.168\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5700, Avg loss: 0.141\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5800, Avg loss: 0.169\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 5900, Avg loss: 0.188\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 6000, Avg loss: 0.156\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 6100, Avg loss: 0.159\n",
      "labelsname: label_Atelectasis, Epoch id: 4, Training steps: 6200, Avg loss: 0.145\n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/validation.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.984, 0.984, 0.984\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.859, 0.859, 0.859\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.738, 0.738, 0.738\n",
      "Acc. (Correct/Total): 0.9499 (31464/33124) \n",
      "Start evaluation on evaluation dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/test.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "The number of evaluation instances:  33124\n",
      "Confusion matrix of {label_name}:\n",
      "tensor([[24693,   309,     8,    75],\n",
      "        [   13,  5923,   192,   782],\n",
      "        [    0,     0,     0,     0],\n",
      "        [    7,   292,    10,   820]])\n",
      "Report precision, recall, and f1:\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.984, 0.984, 0.984\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.857, 0.857, 0.857\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.726, 0.726, 0.726\n",
      "Acc. (Correct/Total): 0.9490 (31436/33124) \n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 100, Avg loss: 0.167\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 200, Avg loss: 0.142\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 300, Avg loss: 0.186\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 400, Avg loss: 0.193\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 500, Avg loss: 0.175\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 600, Avg loss: 0.149\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 700, Avg loss: 0.167\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 800, Avg loss: 0.155\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 900, Avg loss: 0.149\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1000, Avg loss: 0.154\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1100, Avg loss: 0.144\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1200, Avg loss: 0.205\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1300, Avg loss: 0.144\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1400, Avg loss: 0.137\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1500, Avg loss: 0.192\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1600, Avg loss: 0.172\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1700, Avg loss: 0.157\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1800, Avg loss: 0.157\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 1900, Avg loss: 0.191\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2000, Avg loss: 0.147\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2100, Avg loss: 0.171\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2200, Avg loss: 0.157\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2300, Avg loss: 0.125\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2400, Avg loss: 0.160\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2500, Avg loss: 0.169\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2600, Avg loss: 0.152\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2700, Avg loss: 0.193\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2800, Avg loss: 0.158\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 2900, Avg loss: 0.139\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3000, Avg loss: 0.155\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3100, Avg loss: 0.169\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3200, Avg loss: 0.171\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3300, Avg loss: 0.173\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3400, Avg loss: 0.165\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3500, Avg loss: 0.175\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3600, Avg loss: 0.181\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3700, Avg loss: 0.177\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3800, Avg loss: 0.172\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 3900, Avg loss: 0.157\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4000, Avg loss: 0.155\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4100, Avg loss: 0.157\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4200, Avg loss: 0.145\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4300, Avg loss: 0.161\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4400, Avg loss: 0.129\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4500, Avg loss: 0.173\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4600, Avg loss: 0.160\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4700, Avg loss: 0.191\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4800, Avg loss: 0.132\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 4900, Avg loss: 0.150\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5000, Avg loss: 0.140\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5100, Avg loss: 0.185\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5200, Avg loss: 0.121\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5300, Avg loss: 0.140\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5400, Avg loss: 0.168\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5500, Avg loss: 0.203\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5600, Avg loss: 0.163\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5700, Avg loss: 0.136\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5800, Avg loss: 0.160\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 5900, Avg loss: 0.180\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 6000, Avg loss: 0.149\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 6100, Avg loss: 0.155\n",
      "labelsname: label_Atelectasis, Epoch id: 5, Training steps: 6200, Avg loss: 0.140\n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/validation.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.982, 0.982, 0.982\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.863, 0.863, 0.863\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.732, 0.732, 0.732\n",
      "Acc. (Correct/Total): 0.9489 (31432/33124) \n",
      "Final evaluation on the evaluation dataset.\n",
      "Loading sentences from ./datasets/CheXpert/impression/test.csv\n",
      "There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/33124\n",
      "Progress of process 0: 10000/33124\n",
      "Progress of process 0: 20000/33124\n",
      "Progress of process 0: 30000/33124\n",
      "The number of evaluation instances:  33124\n",
      "Confusion matrix of {label_name}:\n",
      "tensor([[24693,   309,     8,    75],\n",
      "        [   13,  5923,   192,   782],\n",
      "        [    0,     0,     0,     0],\n",
      "        [    7,   292,    10,   820]])\n",
      "Report precision, recall, and f1:\n",
      "labelsname: label_Atelectasis, Label_value 0: 0.984, 0.984, 0.984\n",
      "labelsname: label_Atelectasis, Label_value 1: 0.857, 0.857, 0.857\n",
      "model never predicts label value 2\n",
      "labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n",
      "labelsname: label_Atelectasis, Label_value 3: 0.726, 0.726, 0.726\n",
      "Acc. (Correct/Total): 0.9490 (31436/33124) \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 13.98 GB, other allocations: 4.15 GB, max allowed: 18.13 GB). Tried to allocate 48.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m pos_ids_batch \u001b[39m=\u001b[39m pos_ids_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m vms_batch \u001b[39m=\u001b[39m vms_batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 39\u001b[0m loss, _ \u001b[39m=\u001b[39m model(input_ids_batch, label_ids_batch, mask_ids_batch, pos\u001b[39m=\u001b[39;49mpos_ids_batch, vm\u001b[39m=\u001b[39;49mvms_batch)\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     41\u001b[0m     loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(loss)\n",
      "File \u001b[0;32m~/miniforge3/envs/kbert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/BA/infCheXbert_with_gewighted_f1/inf_classifier.py:47\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[0;34m(self, src, label, mask, pos, vm)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_vm:\n\u001b[1;32m     46\u001b[0m     vm \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(emb, mask, vm)\n\u001b[1;32m     48\u001b[0m \u001b[39m# Target.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/kbert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/BA/infCheXbert_with_gewighted_f1/uer/encoders/bert_encoder.py:48\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, emb, seg, vm)\u001b[0m\n\u001b[1;32m     46\u001b[0m hidden \u001b[39m=\u001b[39m emb\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers_num):\n\u001b[0;32m---> 48\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer[i](hidden, mask)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m hidden\n",
      "File \u001b[0;32m~/miniforge3/envs/kbert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/BA/infCheXbert_with_gewighted_f1/uer/layers/transformer.py:40\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, hidden, mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m inter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(hidden, hidden, hidden, mask))\n\u001b[1;32m     39\u001b[0m inter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm_1(inter \u001b[39m+\u001b[39m hidden)\n\u001b[0;32m---> 40\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(inter))\n\u001b[1;32m     41\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm_2(output \u001b[39m+\u001b[39m inter)  \n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniforge3/envs/kbert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/BA/infCheXbert_with_gewighted_f1/uer/layers/position_ffn.py:14\u001b[0m, in \u001b[0;36mPositionwiseFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     inter \u001b[39m=\u001b[39m gelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_1(x))\n\u001b[1;32m     15\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_2(inter)\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/BA/infCheXbert_with_gewighted_f1/uer/utils/act_fun.py:6\u001b[0m, in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgelu\u001b[39m(x):\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39merf(x \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39m2.0\u001b[39;49m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 13.98 GB, other allocations: 4.15 GB, max allowed: 18.13 GB). Tried to allocate 48.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"The number of training instances:\", instances_num)\n",
    "\n",
    "\n",
    "\n",
    "for counter, model in enumerate(models_3_obs):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n",
    "\n",
    "\n",
    "    total_loss = 0.\n",
    "    result = 0.0\n",
    "    best_result = 0.0\n",
    "    label_name = label_names[counter]\n",
    "    label_ids = labels_ids[counter]\n",
    "    for epoch in range(1, args.epochs_num+1):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "            model.zero_grad()\n",
    "\n",
    "            vms_batch = torch.LongTensor(np.array(vms_batch))\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            mask_ids_batch = mask_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                loss = torch.mean(loss)\n",
    "            total_loss += loss.item()\n",
    "            if (i + 1) % args.report_steps == 0:\n",
    "                print(\"labelsname: {}, Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(label_name,epoch, i+1, (total_loss / args.report_steps)))\n",
    "                sys.stdout.flush()\n",
    "                total_loss = 0.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Start evaluation on test dataset.\")         \n",
    "        result = evaluate(args, False,label_id=counter)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            save_model(model, args.output_model_path[counter])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        print(\"Start evaluation on evaluation dataset.\")\n",
    "        evaluate(args, True,label_id=counter)\n",
    "\n",
    "    # Evaluation phase.\n",
    "    print(\"Final evaluation on the evaluation dataset.\")\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model.module.load_state_dict(torch.load(args.output_model_path[counter]))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(args.output_model_path[counter]))\n",
    "    evaluate(args, True,label_id=counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continue training the complete models company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models_rest = models[3:]\n",
    "\n",
    "train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"The number of training instances:\", instances_num)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n",
    "\n",
    "\n",
    "for counter, model in enumerate(models_3_obs):\n",
    "    total_loss = 0.\n",
    "    result = 0.0\n",
    "    best_result = 0.0\n",
    "    label_name = label_names[counter]\n",
    "    label_ids = labels_ids[counter]\n",
    "    for epoch in range(1, args.epochs_num+1):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "            model.zero_grad()\n",
    "\n",
    "            vms_batch = torch.LongTensor(np.array(vms_batch))\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            mask_ids_batch = mask_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                loss = torch.mean(loss)\n",
    "            total_loss += loss.item()\n",
    "            if (i + 1) % args.report_steps == 0:\n",
    "                print(\"labelsname: {}, Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(label_name,epoch, i+1, (total_loss / args.report_steps)))\n",
    "                sys.stdout.flush()\n",
    "                total_loss = 0.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Start evaluation on test dataset.\")         \n",
    "        result = evaluate(args, False,label_id=counter)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            save_model(model, args.output_model_path[counter])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        print(\"Start evaluation on evaluation dataset.\")\n",
    "        evaluate(args, True,label_id=counter)\n",
    "\n",
    "    # Evaluation phase.\n",
    "    print(\"Final evaluation on the evaluation dataset.\")\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model.module.load_state_dict(torch.load(args.output_model_path[counter]))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(args.output_model_path[counter]))\n",
    "    evaluate(args, True,label_id=counter)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
