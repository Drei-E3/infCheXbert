{"cells":[{"cell_type":"markdown","metadata":{"id":"xMSD1eCR2kim"},"source":["# Initialization"]},{"cell_type":"markdown","metadata":{"id":"Vgs-z4w32kin"},"source":["## packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ASy-1tLD2kin"},"outputs":[],"source":["import os\n","working_path = os.getcwd()\n","os.chdir('../')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TOA4ZgDR2kio"},"outputs":[],"source":["import sys\n","import torch\n","import json\n","import random\n","import argparse\n","import collections\n","import torch.nn as nn\n","from uer.utils.vocab import Vocab\n","from uer.utils.constants import *\n","from uer.utils.tokenizer import *\n","from uer.model_builder import build_model\n","from uer.utils.optimizers import  BertAdam\n","from uer.utils.config import load_hyperparam\n","from uer.utils.seed import set_seed\n","from uer.model_saver import save_model\n","from brain import KnowledgeGraph\n","from multiprocessing import Process, Pool\n","import numpy as np\n","import os\n","\n","from transformers import AutoTokenizer\n","from inf_classifier import BertClassifier, add_argument_for_paser_of_BertClassifier"]},{"cell_type":"markdown","metadata":{"id":"RRo2zaBO2kio"},"source":["## set arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7AHaJ_Hv2kio"},"outputs":[],"source":["parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n","add_argument_for_paser_of_BertClassifier(parser)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FsV7ekFa2kio"},"outputs":[],"source":["args=parser.parse_args([\n","  \"--pretrained_model_path\", \"./models/PubMedBERT/PubMedBERT_UER.bin\" ,\n","    \"--config_path\", \"./models/google_config.json\",\n","    \"--vocab_path\", \"./models/PubMedBERT/PubMedBERTvocab.txt\",\n","    # \"--train_path\", \"./datasets/CheXpert/impression/label_split/label_Atelectasis.csv\",\n","    \"--train_path\", \"./datasets/CheXpert/impression/train.csv\",\n","    \"--dev_path\", \"./datasets/CheXpert/impression/validation.csv\",\n","    \"--test_path\", \"./datasets/CheXpert/impression/test.csv\",\n","    \"--epochs_num\", \"3\",\n","    \"--batch_size\", \"16\",\n","   #  \"--tokenizer_from_huggingface\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n","    \"--tokenizer_from_huggingface\", \"\",\n","    \"--kg_name\", \"none\",\n","    \"--output_model_path\", \"./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated.bin\"])\n","\n","\n","args = load_hyperparam(args)\n","args.learning_rate = 2e-5\n","set_seed(args.seed)\n","\n","str2tokenizer = {\"char\": CharTokenizer, \"space\": SpaceTokenizer, \"bert\": BertTokenizer}\n","args.tokenizer = str2tokenizer[args.tokenizer](args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FyQuEfdC2kio"},"outputs":[],"source":["if '.bin' not in args.output_model_path:\n","    print('output models should be saved as .bin file')\n","else:\n","    path = args.output_model_path.split('/')\n","    name_of_modelscompany = path[-1].replace('.bin','')\n","    path = [x + '/' for x in path]\n","    path.remove(path[-1])\n","    Parent_directory = ''.join(path)\n","    if not os.path.exists(Parent_directory):\n","        os.makedirs(Parent_directory)"]},{"cell_type":"markdown","metadata":{"id":"s6nPupnq2kio"},"source":["## initialise global variables"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"it5CuihZ2kio","outputId":"49c62582-5bad-4ab5-910f-ba6cdaa818a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["for english sentences, huggingface tokenizer or nltk tokenizer is recommanded\n","Vocabulary Size:  30522\n","labeler label_Atelectasis would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Atelectasis.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Cardiomegaly would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Cardiomegaly.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Consolidation would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Consolidation.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Edema would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Edema.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Enlarged Cardiomediastinum would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Enlarged Cardiomediastinum.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Fracture would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Fracture.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Lung Lesion would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Lung Lesion.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Lung Opacity would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Lung Opacity.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_No Finding would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_No Finding.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Pleural Effusion would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Pleural Effusion.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Pleural Other would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Pleural Other.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Pneumonia would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Pneumonia.bin\n","[BertClassifier] use visible_matrix: True\n","labeler label_Pneumothorax would be saved under path ./outputs/infCheXbert_simple_integrated/infCheXbert_simple_integrated_label_Pneumothorax.bin\n","[BertClassifier] use visible_matrix: True\n"]}],"source":["columns = {} # to check column locations of labels and text\n","label_columns = {} # to check column location of each label\n","label_names = []\n","labels_sets= [] # a list of store sets of label values, in my case, label values are 0,1,2\n","with open(args.train_path, mode=\"r\", encoding=\"utf-8\") as f:\n","    for line_id, line in enumerate(f):\n","        try:\n","            line = line.strip().split(\"\\t\")\n","            if line_id == 0:\n","                for i, column_name in enumerate(line):\n","                    columns[column_name] = i\n","                    if 'label' in column_name:\n","                        label_columns[column_name] = i\n","                        label_names.append( column_name)\n","                        labels_sets.append(set())\n","                continue\n","            # count label numbers for each label name\n","            # in our case, labels_nums is known: 3 for each label name. following is just for generalization\n","            for i,label_set in enumerate(labels_sets):\n","                label = int(line[label_columns[label_names[i]]])\n","                label_set.add(label)\n","        except:\n","            pass\n","\n","\n","\n","labels_nums = [len(labels_set) for labels_set in labels_sets]\n","\n","# Load vocabulary.\n","vocab = Vocab()\n","print('for english sentences, huggingface tokenizer or nltk tokenizer is recommanded')\n","if args.tokenizer_from_huggingface:\n","    print('huggingface tokenizer detected, use tokenizer {args.tokenizer_from_huggingface}')\n","    tok_en = AutoTokenizer.from_pretrained(args.tokenizer_from_huggingface)\n","vocab.load(args.vocab_path)\n","args.vocab = vocab\n","args.target = \"bert\"\n","\n","device = torch.device('mps' if torch.backends.mps.is_available()\n","                      else \"cuda\" if torch.cuda.is_available()\n","                      else \"cpu\")\n","\n","\n","\n","if torch.backends.mps.is_available(): torch.mps.empty_cache()\n","# Build bert model.\n","models = []\n","output_model_path = []\n","\n","for counter,label_num in enumerate(labels_nums):\n","    args.labels_num = label_num\n","    output_model_path.append(args.output_model_path.replace('.bin','') + '_' + label_names[counter] +'.bin')\n","    print(f'labeler {label_names[counter]} would be saved under path {output_model_path[counter]}')\n","    model = build_model(args)\n","    # Load or initialize parameters.\n","    if args.pretrained_model_path is not None:\n","        # Initialize with pretrained model.\n","        model.load_state_dict(torch.load(args.pretrained_model_path), strict=False)\n","    else:\n","        # Initialize with normal distribution.\n","        for n, p in list(model.named_parameters()):\n","            if 'gamma' not in n and 'beta' not in n:\n","                p.data.normal_(0, 0.02)\n","\n","    model = BertClassifier(args, model)\n","\n","    if torch.cuda.device_count() > 1:\n","        print(\"{} GPUs are available. Let's use them.\".format(torch.cuda.device_count()))\n","        model = nn.DataParallel(model)\n","\n","    model = model.to(device)\n","\n","    models.append(model)\n","\n","args.output_model_path = output_model_path\n"]},{"cell_type":"markdown","metadata":{"id":"3V98BA7N2kip"},"source":["# Build knowledge graph."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7OBEFCLh2kip"},"outputs":[],"source":["if args.kg_name == 'none':\n","    spo_files = []\n","else:\n","    spo_files = [args.kg_name]\n","kg = KnowledgeGraph(spo_files=spo_files, tokenizer=args.tokenizer,predicate=True)"]},{"cell_type":"markdown","metadata":{"id":"KRB0jWqp2kip"},"source":["## define assist functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SAKHfcLs2kip"},"outputs":[],"source":["def add_knowledge_worker(params):\n","    '''\n","    - input parameters are p_id, sentences, columns, label_columns, kg, vocab, args.seq_length\n","    - output is a dataset which is a list\n","        - structure of the dataset output:\n","            output[0]=token_ids,\n","            output[1]=label,\n","            output[2]=mask,\n","            output[3]=pos,\n","            output[4]=vm\n","        - in case of multiple classifier with multiple labels: output[1]=label is again a list\n","            where each entry has all label value for its label name\n","            for example:\n","                output[1][0] == [1,0,2,3,4,...] for columns['label_Atelectasis]\n","                output[1][1] == [1,0,2,3,4,...] for columns['label_Cardiomegaly]\n","\n","\n","    '''\n","\n","    p_id, sentences, columns, kg, vocab, args = params\n","    text_column = {}\n","    labels_columns = {}\n","    for k,v in columns.items():\n","        if 'label' in k:\n","            labels_columns.update({k:v})\n","        else:\n","            text_column.update({k:v})\n","\n","    sentences_num = len(sentences)\n","    dataset = []\n","\n","    labels_position = list(labels_columns.values())\n","    text_position = list(text_column.values())\n","\n","    for line_id, line in enumerate(sentences):\n","        if line_id % 10000 == 0:\n","            print(\"Progress of process {}: {}/{}\".format(p_id, line_id, sentences_num))\n","            sys.stdout.flush()\n","        line = line.strip().split('\\t')\n","        try:\n","            if len(line) == 2:\n","                label = [int(line[columns[\"label\"]])]\n","                text = CLS_TOKEN + line[columns[\"text_a\"]]\n","\n","                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n","                tokens = tokens[0]\n","                pos = pos[0]\n","                vm = vm[0].astype(\"bool\")\n","                if args.tokenizer_from_huggingface:\n","                    token_ids = tok_en.convert_tokens_to_ids(tokens)\n","                else:\n","                    token_ids = [vocab.get(t) for t in tokens]\n","                    # token_ids = args.tokenizer.convert_tokens_to_ids(tokens)\n","\n","                mask = [1 if t != PAD_TOKEN else 0 for t in tokens]\n","\n","                dataset.append((token_ids, label, mask, pos, vm))\n","\n","            elif (len(line) == 3) and (\"text_b\" in line):\n","                label = int(line[columns[\"label\"]])\n","                text = CLS_TOKEN + line[columns[\"text_a\"]] + SEP_TOKEN + line[columns[\"text_b\"]] + SEP_TOKEN\n","\n","                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n","                tokens = tokens[0]\n","                pos = pos[0]\n","                vm = vm[0].astype(\"bool\")\n","\n","                token_ids = [vocab.get(t) for t in tokens]\n","                mask = []\n","                seg_tag = 1\n","                for t in tokens:\n","                    if t == PAD_TOKEN:\n","                        mask.append(0)\n","                    else:\n","                        mask.append(seg_tag)\n","                    if t == SEP_TOKEN:\n","                        seg_tag += 1\n","\n","                dataset.append((token_ids, label, mask, pos, vm))\n","\n","            elif (len(line) == 4) and ('qid' in line):  # for dbqa\n","                qid=int(line[columns[\"qid\"]])\n","                label = int(line[columns[\"label\"]])\n","                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n","                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n","\n","                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n","                tokens = tokens[0]\n","                pos = pos[0]\n","                vm = vm[0].astype(\"bool\")\n","\n","                token_ids = [vocab.get(t) for t in tokens]\n","                mask = []\n","                seg_tag = 1\n","                for t in tokens:\n","                    if t == PAD_TOKEN:\n","                        mask.append(0)\n","                    else:\n","                        mask.append(seg_tag)\n","                    if t == SEP_TOKEN:\n","                        seg_tag += 1\n","\n","                dataset.append((token_ids, label, mask, pos, vm, qid))\n","\n","            # multiple classification with multiple labels\n","\n","            elif len(labels_columns.keys()) >=2 :\n","\n","                labels = [line[x] for x in labels_position]\n","                text = [line[x] for x in text_position]\n","                text = CLS_TOKEN + ' ' +text[0]\n","\n","                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n","                tokens = tokens[0]\n","                pos = pos[0]\n","                vm = vm[0].astype(\"bool\")\n","                # token_ids = [vocab.get(t) for t in tokens]\n","                token_ids = args.tokenizer.convert_tokens_to_ids(tokens)\n","                # token_ids = tok_en.convert_tokens_to_ids(tokens)\n","                mask = [1 if t != PAD_TOKEN else 0 for t in tokens]\n","\n","                dataset.append((token_ids, labels, mask, pos, vm))\n","\n","\n","        except Exception as e:\n","            print(\"Error line: \", line)\n","            print(e)\n","    return dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yQJ5VELy2kip"},"outputs":[],"source":["# batch loader.\n","\n","'''\n","def batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n","    instances_num = input_ids.size()[0]\n","    for i in range(instances_num // batch_size):\n","        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n","        labels_ids_batch = [label_id[i*batch_size: (i+1)*batch_size] for label_id in label_ids]\n","        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n","        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n","        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n","        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n","    if instances_num > instances_num // batch_size * batch_size:\n","        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n","        labels_ids_batch = [label_id[instances_num//batch_size*batch_size:] for label_id in label_ids]\n","        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n","        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n","        vms_batch = vms[instances_num//batch_size*batch_size:]\n","\n","        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n","'''\n","\n","def multi_label_batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n","    instances_num = input_ids.size()[0]\n","    for i in range(instances_num // batch_size):\n","        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n","        labels_ids_batch = [label_id[i*batch_size: (i+1)*batch_size] for label_id in label_ids]\n","        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n","        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n","        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n","        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n","    if instances_num > instances_num // batch_size * batch_size:\n","        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n","        labels_ids_batch = [label_id[instances_num//batch_size*batch_size:] for label_id in label_ids]\n","        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n","        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n","        vms_batch = vms[instances_num//batch_size*batch_size:]\n","        yield input_ids_batch, labels_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n","\n","def batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n","    instances_num = input_ids.size()[0]\n","    for i in range(instances_num // batch_size):\n","        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n","        label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size]\n","        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n","        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n","        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n","        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n","    if instances_num > instances_num // batch_size * batch_size:\n","        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n","        label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n","        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n","        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n","        vms_batch = vms[instances_num//batch_size*batch_size:]\n","        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n","# dataset loader\n","def read_dataset(path, workers_num=1):\n","\n","    print(\"Loading sentences from {}\".format(path))\n","    sentences = []\n","    with open(path, mode='r', encoding=\"utf-8\") as f:\n","        for line_id, line in enumerate(f):\n","            if line_id == 0:\n","                continue\n","            sentences.append(line)\n","    sentence_num = len(sentences)\n","\n","    print(\"There are {} sentence in total. We use {} processes to inject knowledge into sentences.\".format(sentence_num, workers_num))\n","    if workers_num > 1:\n","        params = []\n","        sentence_per_block = int(sentence_num / workers_num) + 1\n","        for i in range(workers_num):\n","            params.append((i, sentences[i*sentence_per_block: (i+1)*sentence_per_block], columns, kg, vocab, args))\n","        pool = Pool(workers_num)\n","        res = pool.map(add_knowledge_worker, params)\n","        pool.close()\n","        pool.join()\n","        dataset = [sample for block in res for sample in block]\n","    else:\n","        params = (0, sentences, columns, kg, vocab, args)\n","        dataset = add_knowledge_worker(params)\n","\n","    return dataset\n","\n","# Evaluation function.\n","def evaluate(args, is_test, metrics='Acc',label_id = -1):\n","    counter = label_id\n","    if is_test:\n","        dataset = read_dataset(args.test_path, workers_num=args.workers_num)\n","    else:\n","        dataset = read_dataset(args.dev_path, workers_num=args.workers_num)\n","    labels_ids=[]\n","    input_ids = torch.LongTensor([sample[0] for sample in dataset])\n","    if label_id == -1:\n","        label_ids = torch.LongTensor([sample[1] for sample in dataset])\n","    for nr, label in enumerate(label_columns):\n","        labels_ids.append(torch.LongTensor([int(example[1][nr]) for example in dataset]))\n","    mask_ids = torch.LongTensor([sample[2] for sample in dataset])\n","    pos_ids = torch.LongTensor([example[3] for example in dataset])\n","    vms = [example[4] for example in dataset]\n","    label_ids=labels_ids[counter]\n","    batch_size = args.batch_size\n","    instances_num = input_ids.size()[0]\n","    if is_test:\n","        print(\"The number of evaluation instances: \", instances_num)\n","\n","    correct = 0\n","    # Confusion matrix.\n","\n","    confusions = [torch.zeros(num, num, dtype=torch.long) for num in labels_nums]\n","    confusion = confusions[counter]\n","    model = models[counter]\n","    label_name =label_names[counter]\n","    label_ids = labels_ids[counter]\n","\n","    model.eval()\n","\n","    if not args.mean_reciprocal_rank:\n","        for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n","\n","            # vms_batch = vms_batch.long()\n","            vms_batch = torch.LongTensor(np.array(vms_batch))\n","\n","            input_ids_batch = input_ids_batch.to(device)\n","            label_ids_batch = label_ids_batch.to(device)\n","            mask_ids_batch = mask_ids_batch.to(device)\n","            pos_ids_batch = pos_ids_batch.to(device)\n","            vms_batch = vms_batch.to(device)\n","\n","            with torch.inference_mode():\n","                try:\n","                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n","                except:\n","                    print(input_ids_batch)\n","                    print(input_ids_batch.size())\n","                    print(vms_batch)\n","                    print(vms_batch.size())\n","\n","            logits = nn.Softmax(dim=1)(logits)\n","            pred = torch.argmax(logits, dim=1).to(device)\n","            gold = label_ids_batch\n","\n","            for j in range(pred.size()[0]):\n","                    confusion[pred[j], gold[j]] += 1\n","            correct += torch.sum(pred == gold).item()\n","\n","        if is_test:\n","            print(\"Confusion matrix of {label_name}:\")\n","            print(confusion)\n","            print(\"Report precision, recall, and f1:\")\n","\n","        for i in range(confusion.size()[0]):\n","\n","            if (confusion[i,:].sum().item() == 0) and (confusion[:,i].sum().item()!= 0):\n","                print(f'model never predicts label value {i}')\n","\n","\n","            elif (confusion[:,i].sum().item()== 0) and (confusion[i,:].sum().item() != 0):\n","                print(f'dataset has no label value {i}')\n","            eps = 1e-9\n","            p = confusion[i,i].item()/(confusion[i,:].sum().item() + eps)\n","            r = confusion[i,i].item()/(confusion[:,i].sum().item() + eps)\n","            f1 = 2*p*r / (p + r + eps)\n","\n","            if i == 1:\n","                label_1_f1 = f1\n","            print(\"labelsname: {}, Label_value {}: {:.3f}, {:.3f}, {:.3f}\".format(label_name,i,p,r,f1))\n","        print(\"Acc. (Correct/Total): {:.4f} ({}/{}) \".format(correct/len(dataset), correct, len(dataset)))\n","        if metrics == 'Acc':\n","            return correct/len(dataset)\n","        elif metrics == 'f1':\n","            return label_1_f1\n","        else:\n","            return correct/len(dataset)\n","    else:\n","            for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n","\n","                vms_batch = torch.LongTensor(np.array(vms_batch))\n","\n","                input_ids_batch = input_ids_batch.to(device)\n","                label_ids_batch = label_ids_batch.to(device)\n","                mask_ids_batch = mask_ids_batch.to(device)\n","                pos_ids_batch = pos_ids_batch.to(device)\n","                vms_batch = vms_batch.to(device)\n","\n","                with torch.no_grad():\n","                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n","                logits = nn.Softmax(dim=1)(logits)\n","                if i == 0:\n","                    logits_all=logits\n","                if i >= 1:\n","                    logits_all=torch.cat((logits_all,logits),0)\n","\n","            order = -1\n","            gold = []\n","            for i in range(len(dataset)):\n","                qid = dataset[i][-1]\n","                label = dataset[i][1]\n","                if qid == order:\n","                    j += 1\n","                    if label == 1:\n","                        gold.append((qid,j))\n","                else:\n","                    order = qid\n","                    j = 0\n","                    if label == 1:\n","                        gold.append((qid,j))\n","\n","            label_order = []\n","            order = -1\n","            for i in range(len(gold)):\n","                if gold[i][0] == order:\n","                    templist.append(gold[i][1])\n","                elif gold[i][0] != order:\n","                    order=gold[i][0]\n","                    if i > 0:\n","                        label_order.append(templist)\n","                    templist = []\n","                    templist.append(gold[i][1])\n","            label_order.append(templist)\n","\n","            order = -1\n","            score_list = []\n","            for i in range(len(logits_all)):\n","                score = float(logits_all[i][1])\n","                qid=int(dataset[i][-1])\n","                if qid == order:\n","                    templist.append(score)\n","                else:\n","                    order = qid\n","                    if i > 0:\n","                        score_list.append(templist)\n","                    templist = []\n","                    templist.append(score)\n","            score_list.append(templist)\n","\n","            rank = []\n","            pred = []\n","            print(len(score_list))\n","            print(len(label_order))\n","            for i in range(len(score_list)):\n","                if len(label_order[i])==1:\n","                    if label_order[i][0] < len(score_list[i]):\n","                        true_score = score_list[i][label_order[i][0]]\n","                        score_list[i].sort(reverse=True)\n","                        for j in range(len(score_list[i])):\n","                            if score_list[i][j] == true_score:\n","                                rank.append(1 / (j + 1))\n","                    else:\n","                        rank.append(0)\n","\n","                else:\n","                    true_rank = len(score_list[i])\n","                    for k in range(len(label_order[i])):\n","                        if label_order[i][k] < len(score_list[i]):\n","                            true_score = score_list[i][label_order[i][k]]\n","                            temp = sorted(score_list[i],reverse=True)\n","                            for j in range(len(temp)):\n","                                if temp[j] == true_score:\n","                                    if j < true_rank:\n","                                        true_rank = j\n","                    if true_rank < len(score_list[i]):\n","                        rank.append(1 / (true_rank + 1))\n","                    else:\n","                        rank.append(0)\n","            MRR = sum(rank) / len(rank)\n","            print(\"MRR\", MRR)\n","            return MRR\n"]},{"cell_type":"markdown","metadata":{"id":"8r4FkqPa2kiq"},"source":["# Preprocess"]},{"cell_type":"markdown","metadata":{"id":"WiiKckdu2kiq"},"source":["## load datasets, integrate KG into datasets and convert to tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iHex7LXe2kiq","outputId":"34a57f05-028c-4dfa-ad05-2d799e5fa214"},"outputs":[{"name":"stdout","output_type":"stream","text":["load training dataset\n","Loading sentences from ./datasets/CheXpert/impression/train.csv\n","There are 99371 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/99371\n","Progress of process 0: 10000/99371\n","Progress of process 0: 20000/99371\n","Progress of process 0: 30000/99371\n","Progress of process 0: 40000/99371\n","Progress of process 0: 50000/99371\n","Progress of process 0: 60000/99371\n","Progress of process 0: 70000/99371\n","Progress of process 0: 80000/99371\n","Progress of process 0: 90000/99371\n","Shuffling dataset\n","Transfer data to tensor, which includes: \n","input_ids\n","label_ids\n","mask_ids\n","pos_ids\n","vms\n"]}],"source":["\n","# Training phase.\n","print(\"load training dataset\")\n","trainset = read_dataset(args.train_path, workers_num=args.workers_num)\n","print(\"Shuffling dataset\")\n","random.shuffle(trainset)\n","instances_num = len(trainset)\n","batch_size = args.batch_size\n","\n","print(\"Transfer data to tensor, which includes: \")\n","print(\"input_ids\")\n","input_ids = torch.LongTensor([example[0] for example in trainset])\n","print(\"label_ids\")\n","labels_ids = []\n","\n","for counter in range(len(label_columns)):\n","    labels_ids.append(torch.LongTensor([int(example[1][counter]) for example in trainset]))\n","\n","'''\n","for counter, label in enumerate(label_columns):\n","    # labels_ids.append(torch.LongTensor([int(example[1][label_columns[label]]) for example in trainset]))\n","    labels_ids.append(torch.LongTensor([int(example[1][counter]) for example in trainset]))\n","'''\n","print(\"mask_ids\")\n","mask_ids = torch.LongTensor([example[2] for example in trainset])\n","print(\"pos_ids\")\n","pos_ids = torch.LongTensor([example[3] for example in trainset])\n","print(\"vms\")\n","vms = [example[4] for example in trainset]"]},{"cell_type":"markdown","metadata":{"id":"aow7SpWF2kiq"},"source":["# train and evaluate"]},{"cell_type":"markdown","metadata":{"id":"P1Ztnz9h2kiq"},"source":["## modelscompany for 3 observations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wYyrGziF2kiq"},"outputs":[],"source":["models_3_obs = models[:3]"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6869735,"status":"ok","timestamp":1694554059199,"user":{"displayName":"Zeming Leng","userId":"08411794886955818032"},"user_tz":-120},"id":"7mX9w_uB2kiq","outputId":"1f352e04-e9b3-46bf-b99c-cd431d65b6d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size:  16\n","The number of training instances: 99371\n"]},{"name":"stderr","output_type":"stream","text":["/content/sample_data/uer/utils/optimizers.py:123: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n"]},{"name":"stdout","output_type":"stream","text":["labelsname: label_Atelectasis, Epoch id: 1, Training steps: 100, Avg loss: 0.975\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 200, Avg loss: 0.677\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 300, Avg loss: 0.555\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 400, Avg loss: 0.441\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 500, Avg loss: 0.368\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 600, Avg loss: 0.269\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 700, Avg loss: 0.310\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 800, Avg loss: 0.237\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 900, Avg loss: 0.247\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1000, Avg loss: 0.278\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1100, Avg loss: 0.235\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1200, Avg loss: 0.302\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1300, Avg loss: 0.250\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1400, Avg loss: 0.222\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1500, Avg loss: 0.269\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1600, Avg loss: 0.248\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1700, Avg loss: 0.244\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1800, Avg loss: 0.253\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 1900, Avg loss: 0.294\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2000, Avg loss: 0.249\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2100, Avg loss: 0.249\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2200, Avg loss: 0.269\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2300, Avg loss: 0.234\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2400, Avg loss: 0.260\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2500, Avg loss: 0.247\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2600, Avg loss: 0.229\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2700, Avg loss: 0.291\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2800, Avg loss: 0.258\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 2900, Avg loss: 0.239\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3000, Avg loss: 0.249\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3100, Avg loss: 0.289\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3200, Avg loss: 0.267\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3300, Avg loss: 0.251\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3400, Avg loss: 0.230\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3500, Avg loss: 0.261\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3600, Avg loss: 0.249\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3700, Avg loss: 0.258\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3800, Avg loss: 0.223\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 3900, Avg loss: 0.243\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4000, Avg loss: 0.229\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4100, Avg loss: 0.224\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4200, Avg loss: 0.206\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4300, Avg loss: 0.239\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4400, Avg loss: 0.183\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4500, Avg loss: 0.218\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4600, Avg loss: 0.215\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4700, Avg loss: 0.237\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4800, Avg loss: 0.192\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 4900, Avg loss: 0.182\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5000, Avg loss: 0.189\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5100, Avg loss: 0.232\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5200, Avg loss: 0.191\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5300, Avg loss: 0.187\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5400, Avg loss: 0.213\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5500, Avg loss: 0.258\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5600, Avg loss: 0.218\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5700, Avg loss: 0.191\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5800, Avg loss: 0.213\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 5900, Avg loss: 0.240\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 6000, Avg loss: 0.220\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 6100, Avg loss: 0.217\n","labelsname: label_Atelectasis, Epoch id: 1, Training steps: 6200, Avg loss: 0.188\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Atelectasis, Label_value 0: 0.985, 0.999, 0.992\n","labelsname: label_Atelectasis, Label_value 1: 0.774, 0.955, 0.855\n","model never predicts label value 2\n","labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n","model never predicts label value 3\n","labelsname: label_Atelectasis, Label_value 3: 0.000, 0.000, 0.000\n","Acc. (Correct/Total): 0.9338 (30931/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[24701,   305,     8,    70],\n","        [   12,  6219,   202,  1607],\n","        [    0,     0,     0,     0],\n","        [    0,     0,     0,     0]])\n","Report precision, recall, and f1:\n","labelsname: label_Atelectasis, Label_value 0: 0.985, 1.000, 0.992\n","labelsname: label_Atelectasis, Label_value 1: 0.774, 0.953, 0.854\n","model never predicts label value 2\n","labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n","model never predicts label value 3\n","labelsname: label_Atelectasis, Label_value 3: 0.000, 0.000, 0.000\n","Acc. (Correct/Total): 0.9335 (30920/33124) \n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 100, Avg loss: 0.211\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 200, Avg loss: 0.199\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 300, Avg loss: 0.228\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 400, Avg loss: 0.231\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 500, Avg loss: 0.230\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 600, Avg loss: 0.205\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 700, Avg loss: 0.207\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 800, Avg loss: 0.189\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 900, Avg loss: 0.193\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1000, Avg loss: 0.208\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1100, Avg loss: 0.186\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1200, Avg loss: 0.248\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1300, Avg loss: 0.187\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1400, Avg loss: 0.181\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1500, Avg loss: 0.227\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1600, Avg loss: 0.224\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1700, Avg loss: 0.178\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1800, Avg loss: 0.201\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 1900, Avg loss: 0.211\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2000, Avg loss: 0.175\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2100, Avg loss: 0.208\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2200, Avg loss: 0.181\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2300, Avg loss: 0.162\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2400, Avg loss: 0.195\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2500, Avg loss: 0.199\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2600, Avg loss: 0.177\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2700, Avg loss: 0.227\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2800, Avg loss: 0.177\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 2900, Avg loss: 0.164\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3000, Avg loss: 0.158\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3100, Avg loss: 0.200\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3200, Avg loss: 0.194\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3300, Avg loss: 0.185\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3400, Avg loss: 0.175\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3500, Avg loss: 0.188\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3600, Avg loss: 0.199\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3700, Avg loss: 0.209\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3800, Avg loss: 0.182\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 3900, Avg loss: 0.190\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4000, Avg loss: 0.176\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4100, Avg loss: 0.178\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4200, Avg loss: 0.197\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4300, Avg loss: 0.185\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4400, Avg loss: 0.150\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4500, Avg loss: 0.209\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4600, Avg loss: 0.169\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4700, Avg loss: 0.209\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4800, Avg loss: 0.153\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 4900, Avg loss: 0.153\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5000, Avg loss: 0.153\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5100, Avg loss: 0.208\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5200, Avg loss: 0.125\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5300, Avg loss: 0.152\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5400, Avg loss: 0.182\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5500, Avg loss: 0.224\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5600, Avg loss: 0.174\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5700, Avg loss: 0.139\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5800, Avg loss: 0.176\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 5900, Avg loss: 0.198\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 6000, Avg loss: 0.170\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 6100, Avg loss: 0.172\n","labelsname: label_Atelectasis, Epoch id: 2, Training steps: 6200, Avg loss: 0.155\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Atelectasis, Label_value 0: 0.984, 0.999, 0.991\n","labelsname: label_Atelectasis, Label_value 1: 0.824, 0.931, 0.874\n","model never predicts label value 2\n","labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n","labelsname: label_Atelectasis, Label_value 3: 0.792, 0.299, 0.434\n","Acc. (Correct/Total): 0.9445 (31285/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[24699,   334,     8,    82],\n","        [   13,  6089,   197,  1068],\n","        [    0,     0,     0,     0],\n","        [    1,   101,     5,   527]])\n","Report precision, recall, and f1:\n","labelsname: label_Atelectasis, Label_value 0: 0.983, 0.999, 0.991\n","labelsname: label_Atelectasis, Label_value 1: 0.827, 0.933, 0.877\n","model never predicts label value 2\n","labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n","labelsname: label_Atelectasis, Label_value 3: 0.831, 0.314, 0.456\n","Acc. (Correct/Total): 0.9454 (31315/33124) \n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 100, Avg loss: 0.182\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 200, Avg loss: 0.156\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 300, Avg loss: 0.196\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 400, Avg loss: 0.199\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 500, Avg loss: 0.194\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 600, Avg loss: 0.162\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 700, Avg loss: 0.192\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 800, Avg loss: 0.164\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 900, Avg loss: 0.152\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1000, Avg loss: 0.169\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1100, Avg loss: 0.161\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1200, Avg loss: 0.213\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1300, Avg loss: 0.153\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1400, Avg loss: 0.151\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1500, Avg loss: 0.207\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1600, Avg loss: 0.181\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1700, Avg loss: 0.166\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1800, Avg loss: 0.169\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 1900, Avg loss: 0.203\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2000, Avg loss: 0.153\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2100, Avg loss: 0.178\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2200, Avg loss: 0.163\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2300, Avg loss: 0.132\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2400, Avg loss: 0.158\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2500, Avg loss: 0.181\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2600, Avg loss: 0.160\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2700, Avg loss: 0.206\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2800, Avg loss: 0.168\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 2900, Avg loss: 0.149\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3000, Avg loss: 0.161\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3100, Avg loss: 0.174\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3200, Avg loss: 0.189\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3300, Avg loss: 0.179\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3400, Avg loss: 0.168\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3500, Avg loss: 0.178\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3600, Avg loss: 0.185\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3700, Avg loss: 0.192\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3800, Avg loss: 0.179\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 3900, Avg loss: 0.177\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4000, Avg loss: 0.176\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4100, Avg loss: 0.169\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4200, Avg loss: 0.162\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4300, Avg loss: 0.173\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4400, Avg loss: 0.141\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4500, Avg loss: 0.183\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4600, Avg loss: 0.166\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4700, Avg loss: 0.193\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4800, Avg loss: 0.144\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 4900, Avg loss: 0.147\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5000, Avg loss: 0.142\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5100, Avg loss: 0.182\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5200, Avg loss: 0.115\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5300, Avg loss: 0.156\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5400, Avg loss: 0.181\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5500, Avg loss: 0.204\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5600, Avg loss: 0.171\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5700, Avg loss: 0.138\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5800, Avg loss: 0.166\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 5900, Avg loss: 0.184\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 6000, Avg loss: 0.159\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 6100, Avg loss: 0.159\n","labelsname: label_Atelectasis, Epoch id: 3, Training steps: 6200, Avg loss: 0.143\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Atelectasis, Label_value 0: 0.983, 0.999, 0.991\n","labelsname: label_Atelectasis, Label_value 1: 0.836, 0.928, 0.880\n","model never predicts label value 2\n","labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n","labelsname: label_Atelectasis, Label_value 3: 0.790, 0.360, 0.494\n","Acc. (Correct/Total): 0.9467 (31358/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[24695,   318,     9,    95],\n","        [   14,  6041,   201,   963],\n","        [    0,     0,     0,     0],\n","        [    4,   165,     0,   619]])\n","Report precision, recall, and f1:\n","labelsname: label_Atelectasis, Label_value 0: 0.983, 0.999, 0.991\n","labelsname: label_Atelectasis, Label_value 1: 0.837, 0.926, 0.879\n","model never predicts label value 2\n","labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n","labelsname: label_Atelectasis, Label_value 3: 0.786, 0.369, 0.502\n","Acc. (Correct/Total): 0.9466 (31355/33124) \n","Final evaluation on the evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[24695,   318,     9,    95],\n","        [   14,  6041,   201,   963],\n","        [    0,     0,     0,     0],\n","        [    4,   165,     0,   619]])\n","Report precision, recall, and f1:\n","labelsname: label_Atelectasis, Label_value 0: 0.983, 0.999, 0.991\n","labelsname: label_Atelectasis, Label_value 1: 0.837, 0.926, 0.879\n","model never predicts label value 2\n","labelsname: label_Atelectasis, Label_value 2: 0.000, 0.000, 0.000\n","labelsname: label_Atelectasis, Label_value 3: 0.786, 0.369, 0.502\n","Acc. (Correct/Total): 0.9466 (31355/33124) \n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 100, Avg loss: 0.985\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 200, Avg loss: 0.728\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 300, Avg loss: 0.678\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 400, Avg loss: 0.609\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 500, Avg loss: 0.630\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 600, Avg loss: 0.588\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 700, Avg loss: 0.485\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 800, Avg loss: 0.475\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 900, Avg loss: 0.481\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1000, Avg loss: 0.441\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1100, Avg loss: 0.368\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1200, Avg loss: 0.340\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1300, Avg loss: 0.406\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1400, Avg loss: 0.359\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1500, Avg loss: 0.372\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1600, Avg loss: 0.326\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1700, Avg loss: 0.335\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1800, Avg loss: 0.376\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 1900, Avg loss: 0.425\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2000, Avg loss: 0.287\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2100, Avg loss: 0.339\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2200, Avg loss: 0.352\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2300, Avg loss: 0.292\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2400, Avg loss: 0.305\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2500, Avg loss: 0.337\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2600, Avg loss: 0.271\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2700, Avg loss: 0.275\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2800, Avg loss: 0.256\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 2900, Avg loss: 0.219\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3000, Avg loss: 0.257\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3100, Avg loss: 0.250\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3200, Avg loss: 0.245\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3300, Avg loss: 0.283\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3400, Avg loss: 0.248\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3500, Avg loss: 0.235\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3600, Avg loss: 0.232\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3700, Avg loss: 0.203\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3800, Avg loss: 0.219\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 3900, Avg loss: 0.215\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4000, Avg loss: 0.268\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4100, Avg loss: 0.235\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4200, Avg loss: 0.221\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4300, Avg loss: 0.217\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4400, Avg loss: 0.208\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4500, Avg loss: 0.209\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4600, Avg loss: 0.235\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4700, Avg loss: 0.232\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4800, Avg loss: 0.236\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 4900, Avg loss: 0.215\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5000, Avg loss: 0.196\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5100, Avg loss: 0.298\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5200, Avg loss: 0.194\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5300, Avg loss: 0.202\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5400, Avg loss: 0.204\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5500, Avg loss: 0.178\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5600, Avg loss: 0.229\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5700, Avg loss: 0.203\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5800, Avg loss: 0.234\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 5900, Avg loss: 0.196\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 6000, Avg loss: 0.207\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 6100, Avg loss: 0.230\n","labelsname: label_Cardiomegaly, Epoch id: 1, Training steps: 6200, Avg loss: 0.192\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Cardiomegaly, Label_value 0: 0.971, 0.987, 0.979\n","labelsname: label_Cardiomegaly, Label_value 1: 0.958, 0.827, 0.888\n","labelsname: label_Cardiomegaly, Label_value 2: 0.735, 0.927, 0.820\n","labelsname: label_Cardiomegaly, Label_value 3: 0.677, 0.576, 0.623\n","Acc. (Correct/Total): 0.9445 (31285/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[23895,   430,    62,   233],\n","        [   48,  4990,    71,    86],\n","        [  196,   502,  1933,    21],\n","        [   16,   179,    23,   439]])\n","Report precision, recall, and f1:\n","labelsname: label_Cardiomegaly, Label_value 0: 0.971, 0.989, 0.980\n","labelsname: label_Cardiomegaly, Label_value 1: 0.961, 0.818, 0.883\n","labelsname: label_Cardiomegaly, Label_value 2: 0.729, 0.925, 0.815\n","labelsname: label_Cardiomegaly, Label_value 3: 0.668, 0.564, 0.611\n","Acc. (Correct/Total): 0.9436 (31257/33124) \n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 100, Avg loss: 0.282\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 200, Avg loss: 0.188\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 300, Avg loss: 0.225\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 400, Avg loss: 0.206\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 500, Avg loss: 0.229\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 600, Avg loss: 0.216\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 700, Avg loss: 0.206\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 800, Avg loss: 0.190\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 900, Avg loss: 0.218\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1000, Avg loss: 0.215\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1100, Avg loss: 0.218\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1200, Avg loss: 0.185\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1300, Avg loss: 0.193\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1400, Avg loss: 0.180\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1500, Avg loss: 0.211\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1600, Avg loss: 0.165\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1700, Avg loss: 0.203\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1800, Avg loss: 0.239\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 1900, Avg loss: 0.208\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2000, Avg loss: 0.147\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2100, Avg loss: 0.231\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2200, Avg loss: 0.218\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2300, Avg loss: 0.192\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2400, Avg loss: 0.181\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2500, Avg loss: 0.226\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2600, Avg loss: 0.179\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2700, Avg loss: 0.177\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2800, Avg loss: 0.197\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 2900, Avg loss: 0.178\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3000, Avg loss: 0.181\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3100, Avg loss: 0.189\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3200, Avg loss: 0.201\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3300, Avg loss: 0.219\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3400, Avg loss: 0.207\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3500, Avg loss: 0.190\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3600, Avg loss: 0.190\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3700, Avg loss: 0.181\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3800, Avg loss: 0.183\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 3900, Avg loss: 0.167\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4000, Avg loss: 0.222\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4100, Avg loss: 0.202\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4200, Avg loss: 0.202\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4300, Avg loss: 0.197\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4400, Avg loss: 0.190\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4500, Avg loss: 0.187\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4600, Avg loss: 0.224\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4700, Avg loss: 0.188\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4800, Avg loss: 0.208\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 4900, Avg loss: 0.186\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5000, Avg loss: 0.166\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5100, Avg loss: 0.228\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5200, Avg loss: 0.172\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5300, Avg loss: 0.179\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5400, Avg loss: 0.177\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5500, Avg loss: 0.160\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5600, Avg loss: 0.198\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5700, Avg loss: 0.176\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5800, Avg loss: 0.211\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 5900, Avg loss: 0.167\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 6000, Avg loss: 0.179\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 6100, Avg loss: 0.214\n","labelsname: label_Cardiomegaly, Epoch id: 2, Training steps: 6200, Avg loss: 0.181\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Cardiomegaly, Label_value 0: 0.983, 0.976, 0.980\n","labelsname: label_Cardiomegaly, Label_value 1: 0.891, 0.887, 0.889\n","labelsname: label_Cardiomegaly, Label_value 2: 0.730, 0.929, 0.818\n","labelsname: label_Cardiomegaly, Label_value 3: 0.891, 0.494, 0.636\n","Acc. (Correct/Total): 0.9454 (31314/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[23623,   223,    36,   151],\n","        [  303,  5363,    96,   229],\n","        [  218,   501,  1942,    23],\n","        [   11,    14,    15,   376]])\n","Report precision, recall, and f1:\n","labelsname: label_Cardiomegaly, Label_value 0: 0.983, 0.978, 0.980\n","labelsname: label_Cardiomegaly, Label_value 1: 0.895, 0.879, 0.887\n","labelsname: label_Cardiomegaly, Label_value 2: 0.724, 0.930, 0.814\n","labelsname: label_Cardiomegaly, Label_value 3: 0.904, 0.483, 0.629\n","Acc. (Correct/Total): 0.9451 (31304/33124) \n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 100, Avg loss: 0.253\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 200, Avg loss: 0.161\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 300, Avg loss: 0.218\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 400, Avg loss: 0.179\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 500, Avg loss: 0.221\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 600, Avg loss: 0.196\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 700, Avg loss: 0.173\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 800, Avg loss: 0.157\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 900, Avg loss: 0.196\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1000, Avg loss: 0.192\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1100, Avg loss: 0.195\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1200, Avg loss: 0.173\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1300, Avg loss: 0.172\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1400, Avg loss: 0.169\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1500, Avg loss: 0.196\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1600, Avg loss: 0.160\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1700, Avg loss: 0.192\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1800, Avg loss: 0.222\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 1900, Avg loss: 0.187\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2000, Avg loss: 0.138\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2100, Avg loss: 0.222\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2200, Avg loss: 0.204\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2300, Avg loss: 0.170\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2400, Avg loss: 0.165\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2500, Avg loss: 0.219\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2600, Avg loss: 0.159\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2700, Avg loss: 0.161\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2800, Avg loss: 0.174\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 2900, Avg loss: 0.152\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3000, Avg loss: 0.168\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3100, Avg loss: 0.168\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3200, Avg loss: 0.181\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3300, Avg loss: 0.207\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3400, Avg loss: 0.195\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3500, Avg loss: 0.174\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3600, Avg loss: 0.178\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3700, Avg loss: 0.167\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3800, Avg loss: 0.187\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 3900, Avg loss: 0.149\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4000, Avg loss: 0.200\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4100, Avg loss: 0.199\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4200, Avg loss: 0.183\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4300, Avg loss: 0.184\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4400, Avg loss: 0.178\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4500, Avg loss: 0.173\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4600, Avg loss: 0.214\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4700, Avg loss: 0.164\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4800, Avg loss: 0.192\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 4900, Avg loss: 0.169\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5000, Avg loss: 0.156\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5100, Avg loss: 0.211\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5200, Avg loss: 0.161\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5300, Avg loss: 0.171\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5400, Avg loss: 0.160\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5500, Avg loss: 0.152\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5600, Avg loss: 0.187\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5700, Avg loss: 0.162\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5800, Avg loss: 0.199\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 5900, Avg loss: 0.159\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 6000, Avg loss: 0.173\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 6100, Avg loss: 0.196\n","labelsname: label_Cardiomegaly, Epoch id: 3, Training steps: 6200, Avg loss: 0.170\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Cardiomegaly, Label_value 0: 0.983, 0.979, 0.981\n","labelsname: label_Cardiomegaly, Label_value 1: 0.911, 0.886, 0.898\n","labelsname: label_Cardiomegaly, Label_value 2: 0.737, 0.926, 0.821\n","labelsname: label_Cardiomegaly, Label_value 3: 0.870, 0.569, 0.688\n","Acc. (Correct/Total): 0.9489 (31430/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[23669,   234,    42,   160],\n","        [  261,  5353,    95,   171],\n","        [  207,   489,  1935,    22],\n","        [   18,    25,    17,   426]])\n","Report precision, recall, and f1:\n","labelsname: label_Cardiomegaly, Label_value 0: 0.982, 0.980, 0.981\n","labelsname: label_Cardiomegaly, Label_value 1: 0.910, 0.877, 0.894\n","labelsname: label_Cardiomegaly, Label_value 2: 0.729, 0.926, 0.816\n","labelsname: label_Cardiomegaly, Label_value 3: 0.877, 0.547, 0.674\n","Acc. (Correct/Total): 0.9474 (31383/33124) \n","Final evaluation on the evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[23669,   234,    42,   160],\n","        [  261,  5353,    95,   171],\n","        [  207,   489,  1935,    22],\n","        [   18,    25,    17,   426]])\n","Report precision, recall, and f1:\n","labelsname: label_Cardiomegaly, Label_value 0: 0.982, 0.980, 0.981\n","labelsname: label_Cardiomegaly, Label_value 1: 0.910, 0.877, 0.894\n","labelsname: label_Cardiomegaly, Label_value 2: 0.729, 0.926, 0.816\n","labelsname: label_Cardiomegaly, Label_value 3: 0.877, 0.547, 0.674\n","Acc. (Correct/Total): 0.9474 (31383/33124) \n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 100, Avg loss: 0.730\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 200, Avg loss: 0.481\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 300, Avg loss: 0.421\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 400, Avg loss: 0.375\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 500, Avg loss: 0.435\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 600, Avg loss: 0.303\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 700, Avg loss: 0.210\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 800, Avg loss: 0.168\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 900, Avg loss: 0.172\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1000, Avg loss: 0.132\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1100, Avg loss: 0.148\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1200, Avg loss: 0.153\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1300, Avg loss: 0.148\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1400, Avg loss: 0.157\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1500, Avg loss: 0.169\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1600, Avg loss: 0.115\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1700, Avg loss: 0.125\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1800, Avg loss: 0.161\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 1900, Avg loss: 0.114\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2000, Avg loss: 0.122\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2100, Avg loss: 0.091\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2200, Avg loss: 0.127\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2300, Avg loss: 0.132\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2400, Avg loss: 0.107\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2500, Avg loss: 0.148\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2600, Avg loss: 0.122\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2700, Avg loss: 0.124\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2800, Avg loss: 0.118\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 2900, Avg loss: 0.123\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3000, Avg loss: 0.120\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3100, Avg loss: 0.120\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3200, Avg loss: 0.141\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3300, Avg loss: 0.124\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3400, Avg loss: 0.109\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3500, Avg loss: 0.118\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3600, Avg loss: 0.138\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3700, Avg loss: 0.127\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3800, Avg loss: 0.124\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 3900, Avg loss: 0.122\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4000, Avg loss: 0.106\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4100, Avg loss: 0.127\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4200, Avg loss: 0.101\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4300, Avg loss: 0.114\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4400, Avg loss: 0.101\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4500, Avg loss: 0.134\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4600, Avg loss: 0.153\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4700, Avg loss: 0.088\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4800, Avg loss: 0.119\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 4900, Avg loss: 0.115\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5000, Avg loss: 0.120\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5100, Avg loss: 0.118\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5200, Avg loss: 0.118\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5300, Avg loss: 0.114\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5400, Avg loss: 0.130\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5500, Avg loss: 0.116\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5600, Avg loss: 0.111\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5700, Avg loss: 0.113\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5800, Avg loss: 0.132\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 5900, Avg loss: 0.124\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 6000, Avg loss: 0.119\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 6100, Avg loss: 0.111\n","labelsname: label_Consolidation, Epoch id: 1, Training steps: 6200, Avg loss: 0.105\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Consolidation, Label_value 0: 0.993, 1.000, 0.996\n","labelsname: label_Consolidation, Label_value 1: 0.714, 0.841, 0.772\n","labelsname: label_Consolidation, Label_value 2: 0.774, 0.848, 0.809\n","model never predicts label value 3\n","labelsname: label_Consolidation, Label_value 3: 0.000, 0.000, 0.000\n","Acc. (Correct/Total): 0.9673 (32040/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[29565,   158,    39,    12],\n","        [    6,  1420,   144,   458],\n","        [    3,   111,  1020,   188],\n","        [    0,     0,     0,     0]])\n","Report precision, recall, and f1:\n","labelsname: label_Consolidation, Label_value 0: 0.993, 1.000, 0.996\n","labelsname: label_Consolidation, Label_value 1: 0.700, 0.841, 0.764\n","labelsname: label_Consolidation, Label_value 2: 0.772, 0.848, 0.808\n","model never predicts label value 3\n","labelsname: label_Consolidation, Label_value 3: 0.000, 0.000, 0.000\n","Acc. (Correct/Total): 0.9662 (32005/33124) \n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 100, Avg loss: 0.117\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 200, Avg loss: 0.113\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 300, Avg loss: 0.133\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 400, Avg loss: 0.099\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 500, Avg loss: 0.116\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 600, Avg loss: 0.136\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 700, Avg loss: 0.123\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 800, Avg loss: 0.128\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 900, Avg loss: 0.129\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1000, Avg loss: 0.103\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1100, Avg loss: 0.119\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1200, Avg loss: 0.120\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1300, Avg loss: 0.125\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1400, Avg loss: 0.111\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1500, Avg loss: 0.132\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1600, Avg loss: 0.092\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1700, Avg loss: 0.096\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1800, Avg loss: 0.123\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 1900, Avg loss: 0.108\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2000, Avg loss: 0.115\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2100, Avg loss: 0.078\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2200, Avg loss: 0.114\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2300, Avg loss: 0.116\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2400, Avg loss: 0.096\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2500, Avg loss: 0.123\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2600, Avg loss: 0.104\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2700, Avg loss: 0.111\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2800, Avg loss: 0.107\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 2900, Avg loss: 0.102\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3000, Avg loss: 0.104\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3100, Avg loss: 0.109\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3200, Avg loss: 0.123\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3300, Avg loss: 0.112\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3400, Avg loss: 0.102\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3500, Avg loss: 0.108\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3600, Avg loss: 0.124\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3700, Avg loss: 0.114\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3800, Avg loss: 0.111\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 3900, Avg loss: 0.108\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4000, Avg loss: 0.089\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4100, Avg loss: 0.102\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4200, Avg loss: 0.095\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4300, Avg loss: 0.102\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4400, Avg loss: 0.091\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4500, Avg loss: 0.124\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4600, Avg loss: 0.119\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4700, Avg loss: 0.071\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4800, Avg loss: 0.101\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 4900, Avg loss: 0.098\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5000, Avg loss: 0.114\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5100, Avg loss: 0.109\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5200, Avg loss: 0.112\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5300, Avg loss: 0.096\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5400, Avg loss: 0.121\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5500, Avg loss: 0.096\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5600, Avg loss: 0.104\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5700, Avg loss: 0.092\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5800, Avg loss: 0.115\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 5900, Avg loss: 0.113\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 6000, Avg loss: 0.108\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 6100, Avg loss: 0.095\n","labelsname: label_Consolidation, Epoch id: 2, Training steps: 6200, Avg loss: 0.089\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Consolidation, Label_value 0: 0.993, 1.000, 0.996\n","labelsname: label_Consolidation, Label_value 1: 0.736, 0.819, 0.775\n","labelsname: label_Consolidation, Label_value 2: 0.818, 0.866, 0.841\n","labelsname: label_Consolidation, Label_value 3: 0.745, 0.182, 0.293\n","Acc. (Correct/Total): 0.9702 (32138/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[29566,   159,    40,    12],\n","        [    5,  1407,   114,   436],\n","        [    3,   106,  1036,    82],\n","        [    0,    17,    13,   128]])\n","Report precision, recall, and f1:\n","labelsname: label_Consolidation, Label_value 0: 0.993, 1.000, 0.996\n","labelsname: label_Consolidation, Label_value 1: 0.717, 0.833, 0.771\n","labelsname: label_Consolidation, Label_value 2: 0.844, 0.861, 0.853\n","labelsname: label_Consolidation, Label_value 3: 0.810, 0.195, 0.314\n","Acc. (Correct/Total): 0.9702 (32137/33124) \n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 100, Avg loss: 0.101\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 200, Avg loss: 0.098\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 300, Avg loss: 0.113\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 400, Avg loss: 0.088\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 500, Avg loss: 0.099\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 600, Avg loss: 0.110\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 700, Avg loss: 0.109\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 800, Avg loss: 0.107\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 900, Avg loss: 0.115\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1000, Avg loss: 0.091\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1100, Avg loss: 0.108\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1200, Avg loss: 0.108\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1300, Avg loss: 0.093\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1400, Avg loss: 0.089\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1500, Avg loss: 0.120\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1600, Avg loss: 0.081\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1700, Avg loss: 0.084\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1800, Avg loss: 0.106\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 1900, Avg loss: 0.087\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2000, Avg loss: 0.097\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2100, Avg loss: 0.066\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2200, Avg loss: 0.099\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2300, Avg loss: 0.104\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2400, Avg loss: 0.077\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2500, Avg loss: 0.103\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2600, Avg loss: 0.083\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2700, Avg loss: 0.103\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2800, Avg loss: 0.094\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 2900, Avg loss: 0.088\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3000, Avg loss: 0.082\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3100, Avg loss: 0.088\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3200, Avg loss: 0.104\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3300, Avg loss: 0.097\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3400, Avg loss: 0.085\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3500, Avg loss: 0.085\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3600, Avg loss: 0.106\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3700, Avg loss: 0.097\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3800, Avg loss: 0.092\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 3900, Avg loss: 0.096\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4000, Avg loss: 0.080\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4100, Avg loss: 0.084\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4200, Avg loss: 0.076\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4300, Avg loss: 0.084\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4400, Avg loss: 0.080\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4500, Avg loss: 0.110\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4600, Avg loss: 0.092\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4700, Avg loss: 0.063\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4800, Avg loss: 0.086\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 4900, Avg loss: 0.078\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5000, Avg loss: 0.094\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5100, Avg loss: 0.085\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5200, Avg loss: 0.098\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5300, Avg loss: 0.078\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5400, Avg loss: 0.104\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5500, Avg loss: 0.083\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5600, Avg loss: 0.087\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5700, Avg loss: 0.079\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5800, Avg loss: 0.101\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 5900, Avg loss: 0.094\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 6000, Avg loss: 0.090\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 6100, Avg loss: 0.076\n","labelsname: label_Consolidation, Epoch id: 3, Training steps: 6200, Avg loss: 0.075\n","Start evaluation on test dataset.\n","Loading sentences from ./datasets/CheXpert/impression/validation.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","labelsname: label_Consolidation, Label_value 0: 0.993, 1.000, 0.996\n","labelsname: label_Consolidation, Label_value 1: 0.787, 0.800, 0.793\n","labelsname: label_Consolidation, Label_value 2: 0.862, 0.854, 0.858\n","labelsname: label_Consolidation, Label_value 3: 0.703, 0.451, 0.550\n","Acc. (Correct/Total): 0.9739 (32259/33124) \n","Start evaluation on evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[29566,   158,    39,    12],\n","        [    6,  1368,   101,   305],\n","        [    2,    79,  1028,    51],\n","        [    0,    84,    35,   290]])\n","Report precision, recall, and f1:\n","labelsname: label_Consolidation, Label_value 0: 0.993, 1.000, 0.996\n","labelsname: label_Consolidation, Label_value 1: 0.769, 0.810, 0.789\n","labelsname: label_Consolidation, Label_value 2: 0.886, 0.855, 0.870\n","labelsname: label_Consolidation, Label_value 3: 0.709, 0.441, 0.544\n","Acc. (Correct/Total): 0.9737 (32252/33124) \n","Final evaluation on the evaluation dataset.\n","Loading sentences from ./datasets/CheXpert/impression/test.csv\n","There are 33124 sentence in total. We use 1 processes to inject knowledge into sentences.\n","Progress of process 0: 0/33124\n","Progress of process 0: 10000/33124\n","Progress of process 0: 20000/33124\n","Progress of process 0: 30000/33124\n","The number of evaluation instances:  33124\n","Confusion matrix of {label_name}:\n","tensor([[29566,   158,    39,    12],\n","        [    6,  1368,   101,   305],\n","        [    2,    79,  1028,    51],\n","        [    0,    84,    35,   290]])\n","Report precision, recall, and f1:\n","labelsname: label_Consolidation, Label_value 0: 0.993, 1.000, 0.996\n","labelsname: label_Consolidation, Label_value 1: 0.769, 0.810, 0.789\n","labelsname: label_Consolidation, Label_value 2: 0.886, 0.855, 0.870\n","labelsname: label_Consolidation, Label_value 3: 0.709, 0.441, 0.544\n","Acc. (Correct/Total): 0.9737 (32252/33124) \n"]}],"source":["train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n","\n","print(\"Batch size: \", batch_size)\n","print(\"The number of training instances:\", instances_num)\n","\n","\n","\n","for counter, model in enumerate(models_3_obs):\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n","    ]\n","    optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n","\n","\n","    total_loss = 0.\n","    result = 0.0\n","    best_result = 0.0\n","    label_name = label_names[counter]\n","    label_ids = labels_ids[counter]\n","    for epoch in range(1, args.epochs_num+1):\n","\n","        model.train()\n","\n","\n","        for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n","            model.zero_grad()\n","\n","            vms_batch = torch.LongTensor(np.array(vms_batch))\n","\n","            input_ids_batch = input_ids_batch.to(device)\n","            label_ids_batch = label_ids_batch.to(device)\n","            mask_ids_batch = mask_ids_batch.to(device)\n","            pos_ids_batch = pos_ids_batch.to(device)\n","            vms_batch = vms_batch.to(device)\n","\n","            loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\n","            if torch.cuda.device_count() > 1:\n","                loss = torch.mean(loss)\n","            total_loss += loss.item()\n","            if (i + 1) % args.report_steps == 0:\n","                print(\"labelsname: {}, Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(label_name,epoch, i+1, (total_loss / args.report_steps)))\n","                sys.stdout.flush()\n","                total_loss = 0.\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(\"Start evaluation on test dataset.\")\n","        result = evaluate(args, False,label_id=counter)\n","        if result > best_result:\n","            best_result = result\n","            save_model(model, args.output_model_path[counter])\n","        else:\n","            continue\n","\n","        print(\"Start evaluation on evaluation dataset.\")\n","        evaluate(args, True,label_id=counter)\n","\n","    # Evaluation phase.\n","    print(\"Final evaluation on the evaluation dataset.\")\n","\n","    if torch.cuda.device_count() > 1:\n","        model.module.load_state_dict(torch.load(args.output_model_path[counter]))\n","    else:\n","        model.load_state_dict(torch.load(args.output_model_path[counter]))\n","    evaluate(args, True,label_id=counter)"]},{"cell_type":"markdown","metadata":{"id":"Zllgmbpp2kiq"},"source":["## continue training the complete models company"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694554059201,"user":{"displayName":"Zeming Leng","userId":"08411794886955818032"},"user_tz":-120},"id":"mNtN_8d-2kiq","outputId":"51abdc9c-03f5-407a-ad1d-059df9205a89"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nmodels_rest = models[3:]\\n\\ntrain_steps = int(instances_num * args.epochs_num / batch_size) + 1\\n\\nprint(\"Batch size: \", batch_size)\\nprint(\"The number of training instances:\", instances_num)\\n\\nparam_optimizer = list(model.named_parameters())\\nno_decay = [\\'bias\\', \\'gamma\\', \\'beta\\']\\noptimizer_grouped_parameters = [\\n            {\\'params\\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \\'weight_decay_rate\\': 0.01},\\n            {\\'params\\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \\'weight_decay_rate\\': 0.0}\\n]\\noptimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\\n\\n\\nfor counter, model in enumerate(models_3_obs):\\n    total_loss = 0.\\n    result = 0.0\\n    best_result = 0.0\\n    label_name = label_names[counter]\\n    label_ids = labels_ids[counter]\\n    for epoch in range(1, args.epochs_num+1):\\n\\n        model.train()\\n\\n\\n        for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\\n            model.zero_grad()\\n\\n            vms_batch = torch.LongTensor(np.array(vms_batch))\\n\\n            input_ids_batch = input_ids_batch.to(device)\\n            label_ids_batch = label_ids_batch.to(device)\\n            mask_ids_batch = mask_ids_batch.to(device)\\n            pos_ids_batch = pos_ids_batch.to(device)\\n            vms_batch = vms_batch.to(device)\\n\\n            loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\\n            if torch.cuda.device_count() > 1:\\n                loss = torch.mean(loss)\\n            total_loss += loss.item()\\n            if (i + 1) % args.report_steps == 0:\\n                print(\"labelsname: {}, Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(label_name,epoch, i+1, (total_loss / args.report_steps)))\\n                sys.stdout.flush()\\n                total_loss = 0.\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n\\n        print(\"Start evaluation on test dataset.\")\\n        result = evaluate(args, False,label_id=counter)\\n        if result > best_result:\\n            best_result = result\\n            save_model(model, args.output_model_path[counter])\\n        else:\\n            continue\\n\\n        print(\"Start evaluation on evaluation dataset.\")\\n        evaluate(args, True,label_id=counter)\\n\\n    # Evaluation phase.\\n    print(\"Final evaluation on the evaluation dataset.\")\\n\\n    if torch.cuda.device_count() > 1:\\n        model.module.load_state_dict(torch.load(args.output_model_path[counter]))\\n    else:\\n        model.load_state_dict(torch.load(args.output_model_path[counter]))\\n    evaluate(args, True,label_id=counter)\\n'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","models_rest = models[3:]\n","\n","train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n","\n","print(\"Batch size: \", batch_size)\n","print(\"The number of training instances:\", instances_num)\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n","]\n","optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n","\n","\n","for counter, model in enumerate(models_3_obs):\n","    total_loss = 0.\n","    result = 0.0\n","    best_result = 0.0\n","    label_name = label_names[counter]\n","    label_ids = labels_ids[counter]\n","    for epoch in range(1, args.epochs_num+1):\n","\n","        model.train()\n","\n","\n","        for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n","            model.zero_grad()\n","\n","            vms_batch = torch.LongTensor(np.array(vms_batch))\n","\n","            input_ids_batch = input_ids_batch.to(device)\n","            label_ids_batch = label_ids_batch.to(device)\n","            mask_ids_batch = mask_ids_batch.to(device)\n","            pos_ids_batch = pos_ids_batch.to(device)\n","            vms_batch = vms_batch.to(device)\n","\n","            loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\n","            if torch.cuda.device_count() > 1:\n","                loss = torch.mean(loss)\n","            total_loss += loss.item()\n","            if (i + 1) % args.report_steps == 0:\n","                print(\"labelsname: {}, Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(label_name,epoch, i+1, (total_loss / args.report_steps)))\n","                sys.stdout.flush()\n","                total_loss = 0.\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(\"Start evaluation on test dataset.\")\n","        result = evaluate(args, False,label_id=counter)\n","        if result > best_result:\n","            best_result = result\n","            save_model(model, args.output_model_path[counter])\n","        else:\n","            continue\n","\n","        print(\"Start evaluation on evaluation dataset.\")\n","        evaluate(args, True,label_id=counter)\n","\n","    # Evaluation phase.\n","    print(\"Final evaluation on the evaluation dataset.\")\n","\n","    if torch.cuda.device_count() > 1:\n","        model.module.load_state_dict(torch.load(args.output_model_path[counter]))\n","    else:\n","        model.load_state_dict(torch.load(args.output_model_path[counter]))\n","    evaluate(args, True,label_id=counter)\n","'''"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
