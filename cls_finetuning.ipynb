{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--no_vm'], dest='no_vm', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='Disable the visible_matrix', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "from uer.utils.vocab import Vocab\n",
    "from uer.utils.constants import *\n",
    "from uer.utils.tokenizer import * \n",
    "from uer.model_builder import build_model\n",
    "from uer.utils.optimizers import  BertAdam\n",
    "from uer.utils.config import load_hyperparam\n",
    "from uer.utils.seed import set_seed\n",
    "from uer.model_saver import save_model\n",
    "from brain import KnowledgeGraph\n",
    "from multiprocessing import Process, Pool\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, args, model):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.embedding = model.embedding\n",
    "        self.encoder = model.encoder\n",
    "        self.labels_num = args.labels_num\n",
    "        self.pooling = args.pooling\n",
    "        self.output_layer_1 = nn.Linear(args.hidden_size, args.hidden_size)\n",
    "        self.output_layer_2 = nn.Linear(args.hidden_size, args.labels_num)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.use_vm = False if args.no_vm else True\n",
    "        print(\"[BertClassifier] use visible_matrix: {}\".format(self.use_vm))\n",
    "\n",
    "    def forward(self, src, label, mask, pos=None, vm=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [batch_size x seq_length]\n",
    "            label: [batch_size]\n",
    "            mask: [batch_size x seq_length]\n",
    "        \"\"\"\n",
    "        # Embedding.\n",
    "        emb = self.embedding(src, mask, pos)\n",
    "        # Encoder.\n",
    "        if not self.use_vm:\n",
    "            vm = None\n",
    "        output = self.encoder(emb, mask, vm)\n",
    "        # Target.\n",
    "        if self.pooling == \"mean\":\n",
    "            output = torch.mean(output, dim=1)\n",
    "        elif self.pooling == \"max\":\n",
    "            output = torch.max(output, dim=1)[0]\n",
    "        elif self.pooling == \"last\":\n",
    "            output = output[:, -1, :]\n",
    "        else:\n",
    "            output = output[:, 0, :]\n",
    "        \n",
    "        output = torch.tanh(self.output_layer_1(output))\n",
    "        logits = self.output_layer_2(output)\n",
    "        loss = self.criterion(self.softmax(logits.view(-1, self.labels_num)), label.view(-1))\n",
    "        return loss, logits\n",
    "\n",
    "\n",
    "def add_knowledge_worker(params):\n",
    "\n",
    "    p_id, sentences, columns, kg, vocab, args = params\n",
    "\n",
    "    sentences_num = len(sentences)\n",
    "    dataset = []\n",
    "    for line_id, line in enumerate(sentences):\n",
    "        if line_id % 10000 == 0:\n",
    "            print(\"Progress of process {}: {}/{}\".format(p_id, line_id, sentences_num))\n",
    "            sys.stdout.flush()\n",
    "        line = line.strip().split('\\t')\n",
    "        try:\n",
    "            if len(line) == 2:\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]]\n",
    "   \n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = [1 if t != PAD_TOKEN else 0 for t in tokens]\n",
    "\n",
    "                dataset.append((token_ids, label, mask, pos, vm))\n",
    "            \n",
    "            elif len(line) == 3:\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]] + SEP_TOKEN + line[columns[\"text_b\"]] + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = []\n",
    "                seg_tag = 1\n",
    "                for t in tokens:\n",
    "                    if t == PAD_TOKEN:\n",
    "                        mask.append(0)\n",
    "                    else:\n",
    "                        mask.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "\n",
    "                dataset.append((token_ids, label, mask, pos, vm))\n",
    "            \n",
    "            elif len(line) == 4:  # for dbqa\n",
    "                qid=int(line[columns[\"qid\"]])\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n",
    "                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = []\n",
    "                seg_tag = 1\n",
    "                for t in tokens:\n",
    "                    if t == PAD_TOKEN:\n",
    "                        mask.append(0)\n",
    "                    else:\n",
    "                        mask.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "                \n",
    "                dataset.append((token_ids, label, mask, pos, vm, qid))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        except:\n",
    "            print(\"Error line: \", line)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Path options.\n",
    "parser.add_argument(\"--pretrained_model_path\", default=None, type=str,\n",
    "                    help=\"Path of the pretrained model.\")\n",
    "parser.add_argument(\"--output_model_path\", default=\"./models/classifier_model.bin\", type=str,\n",
    "                    help=\"Path of the output model.\")\n",
    "parser.add_argument(\"--vocab_path\", default=\"./models/google_vocab.txt\", type=str,\n",
    "                    help=\"Path of the vocabulary file.\")\n",
    "parser.add_argument(\"--train_path\", type=str, required=True,\n",
    "                    help=\"Path of the trainset.\")\n",
    "parser.add_argument(\"--dev_path\", type=str, required=True,\n",
    "                    help=\"Path of the devset.\") \n",
    "parser.add_argument(\"--test_path\", type=str, required=True,\n",
    "                    help=\"Path of the testset.\")\n",
    "parser.add_argument(\"--config_path\", default=\"./models/google_config.json\", type=str,\n",
    "                    help=\"Path of the config file.\")\n",
    "\n",
    "# Model options.\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                    help=\"Batch size.\")\n",
    "parser.add_argument(\"--seq_length\", type=int, default=256,\n",
    "                    help=\"Sequence length.\")\n",
    "parser.add_argument(\"--encoder\", choices=[\"bert\", \"lstm\", \"gru\", \\\n",
    "                                                \"cnn\", \"gatedcnn\", \"attn\", \\\n",
    "                                                \"rcnn\", \"crnn\", \"gpt\", \"bilstm\"], \\\n",
    "                                                default=\"bert\", help=\"Encoder type.\")\n",
    "parser.add_argument(\"--bidirectional\", action=\"store_true\", help=\"Specific to recurrent model.\")\n",
    "parser.add_argument(\"--pooling\", choices=[\"mean\", \"max\", \"first\", \"last\"], default=\"first\",\n",
    "                    help=\"Pooling type.\")\n",
    "\n",
    "# Subword options.\n",
    "parser.add_argument(\"--subword_type\", choices=[\"none\", \"char\"], default=\"none\",\n",
    "                    help=\"Subword feature type.\")\n",
    "parser.add_argument(\"--sub_vocab_path\", type=str, default=\"models/sub_vocab.txt\",\n",
    "                    help=\"Path of the subword vocabulary file.\")\n",
    "parser.add_argument(\"--subencoder\", choices=[\"avg\", \"lstm\", \"gru\", \"cnn\"], default=\"avg\",\n",
    "                    help=\"Subencoder type.\")\n",
    "parser.add_argument(\"--sub_layers_num\", type=int, default=2, help=\"The number of subencoder layers.\")\n",
    "\n",
    "# Tokenizer options.\n",
    "parser.add_argument(\"--tokenizer\", choices=[\"bert\", \"char\", \"word\", \"space\"], default=\"bert\",\n",
    "                    help=\"Specify the tokenizer.\" \n",
    "                            \"Original Google BERT uses bert tokenizer on Chinese corpus.\"\n",
    "                            \"Char tokenizer segments sentences into characters.\"\n",
    "                            \"Word tokenizer supports online word segmentation based on jieba segmentor.\"\n",
    "                            \"Space tokenizer segments sentences into words according to space.\"\n",
    "                            )\n",
    "\n",
    "# Optimizer options.\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-5,\n",
    "                    help=\"Learning rate.\")\n",
    "parser.add_argument(\"--warmup\", type=float, default=0.1,\n",
    "                    help=\"Warm up value.\")\n",
    "\n",
    "# Training options.\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5,\n",
    "                    help=\"Dropout.\")\n",
    "parser.add_argument(\"--epochs_num\", type=int, default=5,\n",
    "                    help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--report_steps\", type=int, default=100,\n",
    "                    help=\"Specific steps to print prompt.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=7,\n",
    "                    help=\"Random seed.\")\n",
    "\n",
    "# Evaluation options.\n",
    "parser.add_argument(\"--mean_reciprocal_rank\", action=\"store_true\", help=\"Evaluation metrics for DBQA dataset.\")\n",
    "\n",
    "# kg\n",
    "parser.add_argument(\"--kg_name\", required=True, help=\"KG name or path\")\n",
    "parser.add_argument(\"--workers_num\", type=int, default=1, help=\"number of process for loading dataset\")\n",
    "parser.add_argument(\"--no_vm\", action=\"store_true\", help=\"Disable the visible_matrix\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=parser.parse_args([\n",
    "    \"--pretrained_model_path\",  \"./models/google_model.bin\",\n",
    "    \"--config_path\",  \"./models/google_config.json\",\n",
    "    \"--vocab_path\",  \"./models/google_vocab.txt\",\n",
    "    \"--train_path\",  \"./datasets/book_review/train.tsv\",\n",
    "    \"--dev_path\",  \"./datasets/book_review/dev.tsv\",\n",
    "    \"--test_path\",  \"./datasets/book_review/test.tsv\",\n",
    "    \"--epochs_num\",  \"5\",\n",
    "    \"--output_model_path\",  \"./outputs/kbert_bookreview_CnDbpedia.bin\",\n",
    "    \"--batch_size\",  \"16\",\n",
    "    \"--kg_name\",  \"CnDbpedia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=parser.parse_args([\n",
    "  \"--pretrained_model_path\", \"./models/PubMedBERT/PubMedBERT_UER.bin\" ,\n",
    "    \"--config_path\", \"./models/google_config.json\", \n",
    "    \"--vocab_path\", \"./models/PubMedBERT/PubMedBERTvocab.txt\", \n",
    "    \"--train_path\", \"./datasets/CheXpert/impression/train.csv\", \n",
    "    \"--dev_path\", \"./datasets/CheXpert/impression/test.csv\",\n",
    "    \"--test_path\", \"./datasets/CheXpert/impression/validation.csv\",\n",
    "    \"--epochs_num\", \"5\", \"--batch_size\", \"16\", \"--kg_name\", \"brain/kgs/kg_anatomy3kAndAtelectasis.spo\",\n",
    "    \"--output_model_path\", \"debug_26_07\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file line 344 has bad format token\n",
      "Vocabulary Size:  21128\n",
      "[BertClassifier] use visible_matrix: True\n"
     ]
    }
   ],
   "source": [
    "# Load the hyperparameters from the config file.\n",
    "args = load_hyperparam(args)\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Count the number of labels.\n",
    "labels_set = set()\n",
    "columns = {}\n",
    "with open(args.train_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line_id, line in enumerate(f):\n",
    "        try:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line_id == 0:\n",
    "                for i, column_name in enumerate(line):\n",
    "                    columns[column_name] = i\n",
    "                continue\n",
    "            label = int(line[columns[\"label\"]])\n",
    "            labels_set.add(label)\n",
    "        except:\n",
    "            pass\n",
    "args.labels_num = len(labels_set) \n",
    "\n",
    "# Load vocabulary.\n",
    "vocab = Vocab()\n",
    "vocab.load(args.vocab_path)\n",
    "args.vocab = vocab\n",
    "\n",
    "# Build bert model.\n",
    "# A pseudo target is added.\n",
    "args.target = \"bert\"\n",
    "model = build_model(args)\n",
    "\n",
    "# Load or initialize parameters.\n",
    "if args.pretrained_model_path is not None:\n",
    "    # Initialize with pretrained model.\n",
    "    model.load_state_dict(torch.load(args.pretrained_model_path), strict=False)  \n",
    "else:\n",
    "    # Initialize with normal distribution.\n",
    "    for n, p in list(model.named_parameters()):\n",
    "        if 'gamma' not in n and 'beta' not in n:\n",
    "            p.data.normal_(0, 0.02)\n",
    "\n",
    "# Build classification model.\n",
    "model = BertClassifier(args, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KnowledgeGraph] Loading spo from /Users/zm_leng/ML/BA/K-BERT/brain/kgs/CnDbpedia.spo\n"
     ]
    }
   ],
   "source": [
    "if args.kg_name == 'none':\n",
    "    spo_files = []\n",
    "else:\n",
    "    spo_files = [args.kg_name]\n",
    "kg = KnowledgeGraph(spo_files=spo_files, predicate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define assist functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datset loader.\n",
    "def batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size]\n",
    "        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n",
    "        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "\n",
    "        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "\n",
    "\n",
    "def read_dataset(path, workers_num=1):\n",
    "\n",
    "    print(\"Loading sentences from {}\".format(path))\n",
    "    sentences = []\n",
    "    with open(path, mode='r', encoding=\"utf-8\") as f:\n",
    "        for line_id, line in enumerate(f):\n",
    "            if line_id == 0:\n",
    "                continue\n",
    "            sentences.append(line)\n",
    "    sentence_num = len(sentences)\n",
    "\n",
    "    print(\"There are {} sentence in total. We use {} processes to inject knowledge into sentences.\".format(sentence_num, workers_num))\n",
    "    if workers_num > 1:\n",
    "        params = []\n",
    "        sentence_per_block = int(sentence_num / workers_num) + 1\n",
    "        for i in range(workers_num):\n",
    "            params.append((i, sentences[i*sentence_per_block: (i+1)*sentence_per_block], columns, kg, vocab, args))\n",
    "        pool = Pool(workers_num)\n",
    "        res = pool.map(add_knowledge_worker, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        dataset = [sample for block in res for sample in block]\n",
    "    else:\n",
    "        params = (0, sentences, columns, kg, vocab, args)\n",
    "        dataset = add_knowledge_worker(params)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Evaluation function.\n",
    "def evaluate(args, is_test, metrics='Acc'):\n",
    "    if is_test:\n",
    "        dataset = read_dataset(args.test_path, workers_num=args.workers_num)\n",
    "    else:\n",
    "        dataset = read_dataset(args.dev_path, workers_num=args.workers_num)\n",
    "\n",
    "    input_ids = torch.LongTensor([sample[0] for sample in dataset])\n",
    "    label_ids = torch.LongTensor([sample[1] for sample in dataset])\n",
    "    mask_ids = torch.LongTensor([sample[2] for sample in dataset])\n",
    "    pos_ids = torch.LongTensor([example[3] for example in dataset])\n",
    "    vms = [example[4] for example in dataset]\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    instances_num = input_ids.size()[0]\n",
    "    if is_test:\n",
    "        print(\"The number of evaluation instances: \", instances_num)\n",
    "\n",
    "    correct = 0\n",
    "    # Confusion matrix.\n",
    "    confusion = torch.zeros(args.labels_num, args.labels_num, dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if not args.mean_reciprocal_rank:\n",
    "        for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "\n",
    "            # vms_batch = vms_batch.long()\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            mask_ids_batch = mask_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n",
    "                except:\n",
    "                    print(input_ids_batch)\n",
    "                    print(input_ids_batch.size())\n",
    "                    print(vms_batch)\n",
    "                    print(vms_batch.size())\n",
    "\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            gold = label_ids_batch\n",
    "            for j in range(pred.size()[0]):\n",
    "                confusion[pred[j], gold[j]] += 1\n",
    "            correct += torch.sum(pred == gold).item()\n",
    "    \n",
    "        if is_test:\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion)\n",
    "            print(\"Report precision, recall, and f1:\")\n",
    "        \n",
    "        for i in range(confusion.size()[0]):\n",
    "            p = confusion[i,i].item()/confusion[i,:].sum().item()\n",
    "            r = confusion[i,i].item()/confusion[:,i].sum().item()\n",
    "            f1 = 2*p*r / (p+r)\n",
    "            if i == 1:\n",
    "                label_1_f1 = f1\n",
    "            print(\"Label {}: {:.3f}, {:.3f}, {:.3f}\".format(i,p,r,f1))\n",
    "        print(\"Acc. (Correct/Total): {:.4f} ({}/{}) \".format(correct/len(dataset), correct, len(dataset)))\n",
    "        if metrics == 'Acc':\n",
    "            return correct/len(dataset)\n",
    "        elif metrics == 'f1':\n",
    "            return label_1_f1\n",
    "        else:\n",
    "            return correct/len(dataset)\n",
    "    else:\n",
    "        for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            mask_ids_batch = mask_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            if i == 0:\n",
    "                logits_all=logits\n",
    "            if i >= 1:\n",
    "                logits_all=torch.cat((logits_all,logits),0)\n",
    "    \n",
    "        order = -1\n",
    "        gold = []\n",
    "        for i in range(len(dataset)):\n",
    "            qid = dataset[i][-1]\n",
    "            label = dataset[i][1]\n",
    "            if qid == order:\n",
    "                j += 1\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "            else:\n",
    "                order = qid\n",
    "                j = 0\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "\n",
    "        label_order = []\n",
    "        order = -1\n",
    "        for i in range(len(gold)):\n",
    "            if gold[i][0] == order:\n",
    "                templist.append(gold[i][1])\n",
    "            elif gold[i][0] != order:\n",
    "                order=gold[i][0]\n",
    "                if i > 0:\n",
    "                    label_order.append(templist)\n",
    "                templist = []\n",
    "                templist.append(gold[i][1])\n",
    "        label_order.append(templist)\n",
    "\n",
    "        order = -1\n",
    "        score_list = []\n",
    "        for i in range(len(logits_all)):\n",
    "            score = float(logits_all[i][1])\n",
    "            qid=int(dataset[i][-1])\n",
    "            if qid == order:\n",
    "                templist.append(score)\n",
    "            else:\n",
    "                order = qid\n",
    "                if i > 0:\n",
    "                    score_list.append(templist)\n",
    "                templist = []\n",
    "                templist.append(score)\n",
    "        score_list.append(templist)\n",
    "\n",
    "        rank = []\n",
    "        pred = []\n",
    "        print(len(score_list))\n",
    "        print(len(label_order))\n",
    "        for i in range(len(score_list)):\n",
    "            if len(label_order[i])==1:\n",
    "                if label_order[i][0] < len(score_list[i]):\n",
    "                    true_score = score_list[i][label_order[i][0]]\n",
    "                    score_list[i].sort(reverse=True)\n",
    "                    for j in range(len(score_list[i])):\n",
    "                        if score_list[i][j] == true_score:\n",
    "                            rank.append(1 / (j + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "\n",
    "            else:\n",
    "                true_rank = len(score_list[i])\n",
    "                for k in range(len(label_order[i])):\n",
    "                    if label_order[i][k] < len(score_list[i]):\n",
    "                        true_score = score_list[i][label_order[i][k]]\n",
    "                        temp = sorted(score_list[i],reverse=True)\n",
    "                        for j in range(len(temp)):\n",
    "                            if temp[j] == true_score:\n",
    "                                if j < true_rank:\n",
    "                                    true_rank = j\n",
    "                if true_rank < len(score_list[i]):\n",
    "                    rank.append(1 / (true_rank + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "        MRR = sum(rank) / len(rank)\n",
    "        print(\"MRR\", MRR)\n",
    "        return MRR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"{} GPUs are available. Let's use them.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "Loading sentences from ./datasets/book_review/train.tsv\n",
      "There are 20000 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/20000\n",
      "Progress of process 0: 10000/20000\n",
      "Shuffling dataset\n",
      "Trans data to tensor.\n",
      "input_ids\n",
      "label_ids\n",
      "mask_ids\n",
      "pos_ids\n",
      "vms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training phase.\n",
    "print(\"Start training.\")\n",
    "trainset = read_dataset(args.train_path, workers_num=args.workers_num)\n",
    "print(\"Shuffling dataset\")\n",
    "random.shuffle(trainset)\n",
    "instances_num = len(trainset)\n",
    "batch_size = args.batch_size\n",
    "\n",
    "print(\"Trans data to tensor.\")\n",
    "print(\"input_ids\")\n",
    "input_ids = torch.LongTensor([example[0] for example in trainset])\n",
    "print(\"label_ids\")\n",
    "label_ids = torch.LongTensor([example[1] for example in trainset])\n",
    "print(\"mask_ids\")\n",
    "mask_ids = torch.LongTensor([example[2] for example in trainset])\n",
    "print(\"pos_ids\")\n",
    "pos_ids = torch.LongTensor([example[3] for example in trainset])\n",
    "print(\"vms\")\n",
    "vms = [example[4] for example in trainset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "    vms_batch = torch.LongTensor(vms_batch)\n",
    "    print(i)\n",
    "    input_ids_batch = input_ids_batch.to(device)\n",
    "    label_ids_batch = label_ids_batch.to(device)\n",
    "    mask_ids_batch = mask_ids_batch.to(device)\n",
    "    pos_ids_batch = pos_ids_batch.to(device)\n",
    "    vms_batch = vms_batch.to(device)\n",
    "    print('input_ids_batch: {input_ids_batch}'+str(input_ids_batch.shape))\n",
    "    print('mask_ids_batch: {mask_ids_batch}'+str(mask_ids_batch.shape))\n",
    "    print('label_ids_batch: {label_ids_batch}' + str(label_ids_batch.shape))\n",
    "    print('pos_ids_batch: {pos_ids_batch}' + str(pos_ids_batch.shape))\n",
    "    print('vms_batch: {vms_batch}' + str(vms_batch.shape))\n",
    "    print(label_ids_batch)\n",
    "    loss, _= model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\n",
    "    print('loss' + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  16\n",
      "The number of training instances: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/pt43vy6x3vl_nb58fb7wn9jh0000gn/T/ipykernel_50876/1453112143.py:23: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  vms_batch = torch.LongTensor(vms_batch)\n",
      "/var/folders/k1/pt43vy6x3vl_nb58fb7wn9jh0000gn/T/ipykernel_50876/1453112143.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  vms_batch = torch.LongTensor(vms_batch)\n",
      "/Users/zm_leng/ML/BA/K-BERT/uer/utils/optimizers.py:123: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch id: 1, Training steps: 100, Avg loss: 0.629\n",
      "Epoch id: 1, Training steps: 200, Avg loss: 0.458\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m         total_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m     39\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 40\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart evaluation on dev dataset.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m result \u001b[39m=\u001b[39m evaluate(args, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/kbert/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/BA/K-BERT/uer/utils/optimizers.py:139\u001b[0m, in \u001b[0;36mBertAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mt_total\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    138\u001b[0m     schedule_fct \u001b[39m=\u001b[39m SCHEDULES[group[\u001b[39m'\u001b[39m\u001b[39mschedule\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m--> 139\u001b[0m     lr_scheduled \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m schedule_fct(state[\u001b[39m'\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m/\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mt_total\u001b[39;49m\u001b[39m'\u001b[39;49m], group[\u001b[39m'\u001b[39;49m\u001b[39mwarmup\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     lr_scheduled \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/ML/BA/K-BERT/uer/utils/optimizers.py:23\u001b[0m, in \u001b[0;36mwarmup_linear\u001b[0;34m(x, warmup)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[39mreturn\u001b[39;00m x\u001b[39m/\u001b[39mwarmup\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1.0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwarmup_linear\u001b[39m(x, warmup\u001b[39m=\u001b[39m\u001b[39m0.002\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39m<\u001b[39m warmup:\n\u001b[1;32m     25\u001b[0m         \u001b[39mreturn\u001b[39;00m x\u001b[39m/\u001b[39mwarmup\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"The number of training instances:\", instances_num)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n",
    "\n",
    "total_loss = 0.\n",
    "result = 0.0\n",
    "best_result = 0.0\n",
    "\n",
    "for epoch in range(1, args.epochs_num+1):\n",
    "    model.train()\n",
    "    for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        label_ids_batch = label_ids_batch.to(device)\n",
    "        mask_ids_batch = mask_ids_batch.to(device)\n",
    "        pos_ids_batch = pos_ids_batch.to(device)\n",
    "        vms_batch = vms_batch.to(device)\n",
    "\n",
    "        loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = torch.mean(loss)\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % args.report_steps == 0:\n",
    "            print(\"Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(epoch, i+1, total_loss / args.report_steps))\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Start evaluation on dev dataset.\")\n",
    "    result = evaluate(args, False)\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        save_model(model, args.output_model_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(\"Start evaluation on test dataset.\")\n",
    "    evaluate(args, True)\n",
    "\n",
    "# Evaluation phase.\n",
    "print(\"Final evaluation on the test dataset.\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(torch.load(args.output_model_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.output_model_path))\n",
    "evaluate(args, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model_hf = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
